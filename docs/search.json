[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "daR4hs",
    "section": "",
    "text": "О курсе\nМатериалы для курса Анализа данных для лингвистов, Школа лингвистики НИУ ВШЭ.",
    "crumbs": [
      "О курсе"
    ]
  },
  {
    "objectID": "index.html#используемые-пакеты",
    "href": "index.html#используемые-пакеты",
    "title": "daR4hs",
    "section": "Используемые пакеты",
    "text": "Используемые пакеты\n\npackageVersion(\"tidyverse\")\n\n[1] '2.0.0'\n\npackageVersion(\"fitdistrplus\")\n\n[1] '1.2.4'\n\npackageVersion(\"mixtools\")\n\n[1] '2.0.0.1'\n\npackageVersion(\"lme4\")\n\n[1] '1.1.38'\n\npackageVersion(\"lmerTest\")\n\n[1] '3.1.3'\n\npackageVersion(\"car\")\n\n[1] '3.1.3'\n\npackageVersion(\"pscl\")\n\n[1] '1.5.9'\n\npackageVersion(\"nnet\")\n\n[1] '7.3.20'\n\npackageVersion(\"MASS\")\n\n[1] '7.3.60.0.1'\n\npackageVersion(\"ggeffects\")\n\n[1] '2.3.1'\n\npackageVersion(\"brms\")\n\n[1] '2.23.0'\n\npackageVersion(\"tidybayes\")\n\n[1] '3.0.7'\n\npackageVersion(\"bayesAB\")\n\n[1] '1.1.3'",
    "crumbs": [
      "О курсе"
    ]
  },
  {
    "objectID": "index.html#домашние-задания",
    "href": "index.html#домашние-задания",
    "title": "daR4hs",
    "section": "Домашние задания",
    "text": "Домашние задания",
    "crumbs": [
      "О курсе"
    ]
  },
  {
    "objectID": "01-distributions.html",
    "href": "01-distributions.html",
    "title": "1  Распределения",
    "section": "",
    "text": "1.1 Распределения в R\nВ R встроено какое-то количество известных распределений. Все они представлены четырьмя функциями:\nРассмотрим все это на примере нормального распределения.\ntibble(x = 1:100,\n       PDF = dnorm(x = x, mean = 50, sd = 10)) |&gt; \n  ggplot(aes(x, PDF))+\n  geom_point()+\n  geom_line()+\n  labs(title = \"PDF нормального распределения (μ = 50, sd = 10)\")\n\n\n\n\n\n\n\ntibble(x = 1:100,\n       CDF = pnorm(x, mean = 50, sd = 10)) |&gt; \n  ggplot(aes(x, CDF))+\n  geom_point()+\n  geom_line()+\n  labs(title = \"CDF нормального распределения (μ = 50, sd = 10)\")\n\n\n\n\n\n\n\ntibble(quantiles = seq(0, 1, by = 0.01),\n       value = qnorm(quantiles, mean = 50, sd = 10)) |&gt; \n  ggplot(aes(quantiles, value))+\n  geom_point()+\n  geom_line()+\n  labs(title = \"inverse CDF нормального распределения (μ = 50, sd = 10)\")\n\n\n\n\n\n\n\ntibble(sample = rnorm(100, mean = 50, sd = 10)) |&gt; \n  ggplot(aes(sample))+\n  geom_histogram()+\n  labs(title = \"выборка нормально распределенных чисел (μ = 50, sd = 10)\")\nЕсли не использовать set.seed(), то результат работы рандомизатора нельзя будет повторить.\nответы:\nнедостаток 1\n\n\nнедостаток 2",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Распределения</span>"
    ]
  },
  {
    "objectID": "01-distributions.html#распределения-в-r",
    "href": "01-distributions.html#распределения-в-r",
    "title": "1  Распределения",
    "section": "",
    "text": "d... (функция плотности, probability density function),\np... (функция распределения, cumulative distribution function) — интеграл площади под кривой от начала до указанной квантили\nq... (обратная функции распределения, inverse cumulative distribution function) — значение p-той квантили распределения\nи r... (рандомные числа из заданного распределения).\n\n\n\n\n\n\n\n\n\n\nКакое значение имеет 25% квантиль нормального распределения со средним в 20 и стандартным отклонением 90? Ответ округлите до трех знаков после запятой.\n\n\n\n\n\n\n\n\n\n\nДанные из базы данных фонетических инвентарей PHOIBLE (Moran, McCloy, and Wright 2014), достаточно сильно упрощая, можно описать нормальным распределением со средним 35 фонем и стандартным отклонением 13. Если мы ничего не знаем про язык, оцените с какой вероятностью, согласно этой модели произвольно взятый язык окажется в промежутке между 25 и 50 фонемами? Ответ округлите до трех знаков после запятой.\n\n\n\n\n\n\n\n\n\n\nКакие есть недостатки у модели из предыдущего задания?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Распределения</span>"
    ]
  },
  {
    "objectID": "01-distributions.html#дискретные-переменные",
    "href": "01-distributions.html#дискретные-переменные",
    "title": "1  Распределения",
    "section": "1.2 Дискретные переменные",
    "text": "1.2 Дискретные переменные\n\n1.2.1 Биномиальное распределение\nБиномиальное распределение — распределение количетсва успехов эксперементов Бернулли из n попыток с вероятностью успеха p.\n\\[P(k | n, p) = \\frac{n!}{k!(n-k)!} \\times p^k \\times (1-p)^{n-k} =  {n \\choose k} \\times p^k \\times (1-p)^{n-k}\\] \\[ 0 \\leq p \\leq 1; n, k &gt; 0\\]\n\ntibble(x = 0:50,\n       density = dbinom(x = x, size = 50, prob = 0.16)) |&gt; \n  ggplot(aes(x, density))+\n  geom_point()+\n  geom_line()+\n  labs(title = \"Биномиальное распределение p = 0.16, n = 50\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nНемного упрощая данные из статьи (Rosenbach 2003: 394), можно сказать что носители британского английского предпочитают s-генитив (90%) of-генитиву (10%). Какова вероятность, согласно этим данным, что в интервью британского актера из 118 контекстов будет 102 s-генитивов? Ответ округлите до трёх ИЛИ МЕНЕЕ знаков после запятой.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nА какое значение количества s-генитивов наиболее ожидаемо, согласно этой модели?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1.2.2 Геометрическое распределение\nГеометрическое распределение — распределение количетсва эксперементов Бернулли с вероятностью успеха p до первого успеха.\n\\[P(k | p) = (1-p)^k\\times p\\] \\[k\\in\\{1, 2, \\dots\\}\\]\n\ntibble(x = 0:50,\n       density = dgeom(x = x, prob = 0.16)) |&gt; \n  ggplot(aes(x, density))+\n  geom_point()+\n  geom_line()+\n  labs(title = \"Геометрическое распределение p = 0.16, n = 50\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nПриняв модель из (Rosenbach 2003: 394), какова вероятность, что в интервью с британским актером первый of-генитив будет третьим по счету?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1.2.3 Распределение Пуассона\nРаспределение дискретной переменной, обозначающей количество случаев \\(k\\) некоторого события, которое происходит с некоторой заданной частотой \\(\\lambda\\).\n\\[P(\\lambda) = \\frac{e^{-\\lambda}\\times\\lambda^k}{k!}\\]\n\ntibble(k = 0:50,\n       density = dpois(x = k, lambda = 5)) |&gt; \n  ggplot(aes(k, density))+\n  geom_point()+\n  geom_line()+\n  labs(title = \"Распределение Пуассона с параметром λ = 5\")\n\n\n\n\n\n\n\n\nПараметр \\(\\lambda\\) в модели Пуассона одновременно является и средним, и дисперсией.\nПопробуем воспользоваться распределением Пуассона для моделирования количества слогов в андийском языке. Количество слогов – это всегда натуральное число (т. е. не бывает 2.5 слогов, не бывает -3 слогов и т. д., но в теории может быть 0 слогов), так что модель Пуассона здесь применима. Согласно модели Пуассона все слова независимо друг от друга получают сколько-то слогов согласно распределению Пуассона. Посмотрим на данные:\n\nandic_syllables &lt;- read_csv(\"https://raw.githubusercontent.com/agricolamz/2021_da4l/master/data/andic_syllables.csv\") \n\nandic_syllables |&gt; \n  ggplot(aes(n_syllables, count))+\n  geom_col()+\n  facet_wrap(~language, scales = \"free\")\n\n\n\n\n\n\n\n\nПтичка напела (мы научимся узнавать, откуда птичка это знает на следующем занятии), что андийские данные можно описать при помощи распределения Пуассона с параметром \\(\\lambda\\) = 2.783.\n\nandic_syllables |&gt; \n  filter(language == \"Andi\") |&gt; \n  rename(observed = count) |&gt; \n  mutate(predicted = dpois(n_syllables, lambda = 2.783)*sum(observed)) |&gt; \n  pivot_longer(names_to = \"type\", values_to = \"value\", cols = c(observed, predicted)) |&gt; \n  ggplot(aes(n_syllables, value, fill = type))+\n  geom_col(position = \"dodge\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nНа графиках ниже представлены предсказания трех Пуассоновских моделей, какая кажется лучше?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nВыше было написано:\n\nСогласно модели Пуассона все слова независимо друг от друга получают сколько-то слогов согласно распределению Пуассона.\n\nКакие проблемы есть у предположения о независимости друг от друга количества слогов разных слов в словаре?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Распределения</span>"
    ]
  },
  {
    "objectID": "01-distributions.html#числовые-переменные",
    "href": "01-distributions.html#числовые-переменные",
    "title": "1  Распределения",
    "section": "1.3 Числовые переменные",
    "text": "1.3 Числовые переменные\n\n1.3.1 Нормальное распределение\n\\[P(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\times e^{-\\frac{\\left(x-\\mu\\right)^2}{2\\sigma^2}}\\]\n\\[\\mu \\in \\mathbb{R}; \\sigma^2 &gt; 0\\]\n\ntibble(x = 1:100,\n       PDF = dnorm(x = x, mean = 50, sd = 10)) |&gt; \n  ggplot(aes(x, PDF))+\n  geom_point()+\n  geom_line()+\n  labs(title = \"PDF нормального распределения (μ = 50, sd = 10)\")\n\n\n\n\n\n\n\n\nПтичка напела, что длительность гласных американского английского из (Hillenbrand et al. 1995) можно описать нормальным распределением с параметрами \\(\\mu =\\) 274.673 и \\(\\sigma =\\) 64.482. Посмотрим, как можно совместить данные и это распределение:\n\nvowels &lt;- read_csv(\"https://raw.githubusercontent.com/agricolamz/2024_HSE_b_da4l/main/data/phonTools_hillenbrand_1995.csv\") \nvowels |&gt; \n  ggplot(aes(dur)) + \n  geom_histogram(aes(y = after_stat(density))) + # обратите внимание на аргумент after_stat(density)\n  stat_function(fun = dnorm, args = list(mean = 274.673, sd = 64.482), color = \"red\")\n\n\n\n\n\n\n\n\n\n\n1.3.2 Логнормальное распределение\n\\[P(x) = \\frac{1}{x\\sqrt{2\\pi\\sigma^2}}\\times e^{-\\frac{\\left(\\ln x-\\mu\\right)^2}{2\\sigma^2}}\\]\n\\[\\mu \\in \\mathbb{R}; \\sigma^2 &gt; 0\\]\n\ntibble(x = 1:100,\n       PDF = dlnorm(x = x, mean = 3, sd = 0.5)) |&gt; \n  ggplot(aes(x, PDF))+\n  geom_point()+\n  geom_line()+\n  labs(title = \"PDF логнормального распределения (μ = 3, σ = 0.5)\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nКакая из логнормальных моделей для длительности гласных американского английского из (Hillenbrand et al. 1995) лучше подходит к данным? Попробуйте самостоятельно построить данный график.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1.3.3 Что еще почитать про распределения?\nЛюди придумали очень много разных распределений. Стоит, наверное, также понимать, что распределения не существуют отдельно в вакууме: многие из них математически связаны друг с другом. Про это можно посмотреть вот здесь или здесь.\n\n\n\n\nHillenbrand, James, Laura A Getty, Michael J Clark, and Kimberlee Wheeler. 1995. “Acoustic Characteristics of American English Vowels.” The Journal of the Acoustical Society of America 97 (5): 3099–3111.\n\n\nMoran, Steven, Daniel McCloy, and Richard Wright, eds. 2014. PHOIBLE Online. Leipzig: Max Planck Institute for Evolutionary Anthropology. https://phoible.org/.\n\n\nRosenbach, Anette. 2003. “Aspects of Iconicity and Economy in the Choice Between the s-Genitive and the of-Genitive in English.” In Determinants of Grammatical Variation in English, edited by Günter Rohdenburg and Britta Mondorf. Berlin, New York: Mouton de Gruyter.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Распределения</span>"
    ]
  },
  {
    "objectID": "02-likelihood.html",
    "href": "02-likelihood.html",
    "title": "2  Метод максимального правдоподобия",
    "section": "",
    "text": "2.1 Оценка вероятности\nlibrary(tidyverse)\nКогда у нас задано некоторое распределение, мы можем задавать к нему разные вопросы. Например, если мы верим что длительность гласных американского английского из (Hillenbrand et al. 1995) можно описать логнормальным распределением с параметрами \\(\\ln{\\mu} =\\) 5.587 и \\(\\ln{\\sigma} =\\) 0.242, то мы можем делать некотрые предсказания относительно интересующей нас переменной.\nggplot() + \n  stat_function(fun = dlnorm, args = list(mean = 5.587, sd = 0.242))+\n  scale_x_continuous(breaks = 0:6*100, limits = c(0, 650))+\n  labs(x = \"длительность гласного (мс)\",\n       y = \"значение функции плотности\")",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Метод максимального правдоподобия</span>"
    ]
  },
  {
    "objectID": "02-likelihood.html#оценка-вероятности",
    "href": "02-likelihood.html#оценка-вероятности",
    "title": "2  Метод максимального правдоподобия",
    "section": "",
    "text": "Если принять на веру, что логнормальное распределение с параметрами \\(\\ln{\\mu} =\\) 5.587 и \\(\\ln{\\sigma}=\\) 0.242 описывает данные длительности гласных американского английского из (Hillenbrand et al. 1995), то какова вероятность наблюдать значения между 300 и 400 мс? То же самое можно записать, используя математическую нотацию:\n\\[P\\left(X \\in [300,\\, 400] | X \\sim \\ln{\\mathcal{N}}(\\ln{\\mu} = 5.587, \\ln{\\sigma}=0.242)\\right) = ??\\] Ответ округлите до трех и меньше знаков после запятой.\n\n\n\n\n\n\n\n\n\n\n\nЕсли принять на веру, что биномиальное распределение с параметрами \\(p =\\) 0.9 описывает, согласно (Rosenbach 2003: 394) употребление s-генитивов в британском английском, то какова вероятность наблюдать значения между 300 и 350 генитивов в интервью, содержащее 400 генитивных контекстов? То же самое можно записать, используя математическую нотацию:\n\\[P\\left(X \\in [300,\\, 350] | X \\sim Binom(n = 400, p = 0.9)\\right) = ??\\] Ответ округлите до трех и меньше знаков после запятой.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Метод максимального правдоподобия</span>"
    ]
  },
  {
    "objectID": "02-likelihood.html#функция-правдоподобия",
    "href": "02-likelihood.html#функция-правдоподобия",
    "title": "2  Метод максимального правдоподобия",
    "section": "2.2 Функция правдоподобия",
    "text": "2.2 Функция правдоподобия\nЕсли при поиске вероятностей, мы предполагали, что данные нам неизвестны, а распределение и его параметры известны, то функция правдоподобия позволяет этот процесс перевернуть, запустив поиск параметров распределения, при изветсных данных и семье распределения:\n\\[L\\left(X \\sim Distr(...)|x\\right) = ...\\]\nТаким образом получается, что на основании функции плотности мы можем сравнивать, какой параметр лучше подходит к нашим данным.\nДля примера рассмотрим наш s-генетив: мы провели интервью и нам встретилось 85 s-генетивов из 100 случаев всех генетивов. Насколько хорошо подходит нам распределение с параметром p = 0.9?\n\n\n\n\n\n\n\n\n\nОтвет:\n\ndbinom(85, 100, 0.9)\n\n[1] 0.03268244\n\n\nПредставим теперь это как функцию от параметра p:\n\ntibble(p = seq(0, 1, by = 0.01)) |&gt; \n  ggplot(aes(p)) +\n  stat_function(fun = function(p) dbinom(85, 100, p), geom = \"col\")+\n  labs(x = \"параметр биномиального распределения p\",\n       y = \"значение функции правдоподобия\\n(одно наблюдение)\")\n\n\n\n\n\n\n\n\nА что если мы располагаем двумя интервью одного актера? В первом на сто генитивов пришлось 85 s-генитивов, а во втором – 89. В таком случае, также как и с вероятностью наступления двух независимых событий, значения функции плотности перемножаются.\n\ndbinom(85, 100, 0.9)*dbinom(89, 100, 0.9)\n\n[1] 0.003917892\n\n\n\ntibble(p = seq(0, 1, by = 0.01)) |&gt; \n  ggplot(aes(p)) +\n  stat_function(fun = function(p) dbinom(85, 100, p)*dbinom(89, 100, p), geom = \"col\")+\n  labs(x = \"параметр биномиального распределения p\",\n       y = \"значение функции правдоподобия\\n(два наблюдения)\")\n\n\n\n\n\n\n\n\nВ итоге:\n\nвероятность — P(data|distribution)\nправдоподобие — L(distribution|data)\n\nИнтеграл распределения/сумма значений вероятностей равен/на 1. Интеграл распределения/сумма значений правдоподобия может быть не равен/на 1.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Метод максимального правдоподобия</span>"
    ]
  },
  {
    "objectID": "02-likelihood.html#пример-с-непрерывным-распределением",
    "href": "02-likelihood.html#пример-с-непрерывным-распределением",
    "title": "2  Метод максимального правдоподобия",
    "section": "2.3 Пример с непрерывным распределением",
    "text": "2.3 Пример с непрерывным распределением\nМы уже обсуждали, что длительность гласных американского английского из (Hillenbrand et al. 1995) можно описать логнормальным распределением с параметрами \\(\\ln\\mu\\) и \\(\\ln\\sigma\\). Предположим, что \\(\\ln\\sigma = 0.342\\), построим функцию правдоподобия для \\(\\ln\\mu\\):\n\nvowels &lt;- read_csv(\"https://raw.githubusercontent.com/agricolamz/2024_HSE_b_da4l/main/data/phonTools_hillenbrand_1995.csv\") \n\ntibble(ln_mu = seq(5, 6, by = 0.001)) |&gt; \n  ggplot(aes(ln_mu)) + \n  stat_function(fun = function(ln_mu) dlnorm(vowels$dur[1], meanlog = ln_mu, sdlog = 0.242))+\n  labs(x = \"параметр логнормального распределения ln μ\",\n       y = \"значение функции правдоподобия\\n(одно наблюдение)\")\n\n\n\n\n\n\n\ntibble(ln_mu = seq(5, 6, by = 0.001)) |&gt; \n  ggplot(aes(ln_mu)) + \n  stat_function(fun = function(ln_mu) dlnorm(vowels$dur[1], meanlog = ln_mu, sdlog = 0.242)*dlnorm(vowels$dur[2], meanlog = ln_mu, sdlog = 0.242))+\n  labs(x = \"параметр логнормального распределения ln μ\",\n       y = \"значение функции правдоподобия\\n(два наблюдения)\")\n\n\n\n\n\n\n\ntibble(ln_mu = seq(5, 6, by = 0.001)) |&gt; \n  ggplot(aes(ln_mu)) + \n  stat_function(fun = function(ln_mu) dlnorm(vowels$dur[1], meanlog = ln_mu, sdlog = 0.242)*dlnorm(vowels$dur[2], meanlog = ln_mu, sdlog = 0.242)*dlnorm(vowels$dur[3], meanlog = ln_mu, sdlog = 0.242))+\n  labs(x = \"параметр логнормального распределения ln μ\",\n       y = \"значение функции правдоподобия\\n(три наблюдения)\")\n\n\n\n\n\n\n\n\nДля простоты в начале я зафиксировал один из параметров логнормального распредления: лог стандартное отклонение. Конечно, это совсем необязательно делать: можно создать матрицу значений лог среднего и лог стандартного отклонения и получить для каждой ячейки матрицы значения функции правдоподобия.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Метод максимального правдоподобия</span>"
    ]
  },
  {
    "objectID": "02-likelihood.html#метод-максимального-правдоподобия-mle",
    "href": "02-likelihood.html#метод-максимального-правдоподобия-mle",
    "title": "2  Метод максимального правдоподобия",
    "section": "2.4 Метод максимального правдоподобия (MLE)",
    "text": "2.4 Метод максимального правдоподобия (MLE)\nФункция правдоподобия позволяет подбирать параметры распределения. Оценка параметров распределения при помощи функции максимального правдоподобия получила название метод максимального правдоподобия. Его я и использовал ранее для того, чтобы получить значения распределений для заданий из первого занятия:\n\nданные длительности американских гласных из (Hillenbrand et al. 1995) и логнормальное распределение\n\n\nfitdistrplus::fitdist(vowels$dur, distr = 'lnorm', method = 'mle')\n\nFitting of the distribution ' lnorm ' by maximum likelihood \nParameters:\n         estimate  Std. Error\nmeanlog 5.5870359 0.005935135\nsdlog   0.2423978 0.004196453\n\n\n\nколичество андийских слогов в словах и распределение Пуассона\n\n\nandic_syllables &lt;- read_csv(\"https://raw.githubusercontent.com/agricolamz/2024_HSE_b_da4l/main/data/andic_syllables.csv\") \n\nandic_syllables |&gt; \n  filter(language == \"Andi\") |&gt; \n  uncount(count) |&gt; \n  pull(n_syllables) |&gt; \n  fitdistrplus::fitdist(distr = 'pois', method = 'mle')\n\nFitting of the distribution ' pois ' by maximum likelihood \nParameters:\n       estimate Std. Error\nlambda 2.782715 0.02128182\n\n\n\nЕсть и другие методы оценки параметров.\nМетод максимального правдоподобия может быть чувствителен к размеру выборки.\n\n\n\n\n\n\n\nОтфильтруйте из данных с количеством слогов в андийских языках багвалинский и, используя метод максимального правдоподобия, оцените для них параметры модели Пуассона.\n\n\n\n\n\n\n\n\n\nВ работе (Coretta 2016) собраны данные длительности исландских гласных. Отфильтруйте данные, оставив односложные слова (переменная syllables) после придыхательного (переменная aspiration), произнесенные носителем tt01 (переменная speaker) и постройте следующий график, моделируя длительность гласных (переменная vowel.dur) нормальным и логнормальным распределением. Как вам кажется, какое распределение лучше подходит к данным? Докажите ваше утверждение, сравнив значения правдоподобия.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Метод максимального правдоподобия</span>"
    ]
  },
  {
    "objectID": "02-likelihood.html#логорифм-функции-правдоподобия",
    "href": "02-likelihood.html#логорифм-функции-правдоподобия",
    "title": "2  Метод максимального правдоподобия",
    "section": "2.5 Логорифм функции правдоподобия",
    "text": "2.5 Логорифм функции правдоподобия\nТак как в большинстве случаев нужно найти лишь максимум функции правдоподобия, а не саму функцию \\(\\ell(x|\\theta)\\), то для облегчения подсчетов используют логорифмическую функцию правдоподобия \\(\\ln\\ell(x|\\theta)\\): в результате, вместо произведения появляется сумма1:\n\\[\\text{argmax}_\\theta \\prod \\ell(\\theta|x) = \\text{argmax}_\\theta \\sum \\ln\\ell(\\theta|x) \\]\nВо всех предыдущих примерах мы смотрели на 1–3 примера данных, давайте попробуем использовать функцию правдоподобия для большего набора данных.\n\n\n\n\n\n\nПредставим, что мы проводим некоторый эксперимент, и у некоторых участников все получается с первой попытки, а некоторым нужна еще одна попытка или даже две. Дополните код функциями правдоподобия и логорифмической функцией правдоподобия, чтобы получился график ниже.\n\n\n\n\nset.seed(42)\nv &lt;- sample(0:2, 10, replace = TRUE)\n\nsapply(seq(0.01, 0.99, 0.01), function(p){\n  ...\n}) -&gt;\n  likelihood\n\nsapply(seq(0.01, 0.99, 0.01), function(p){\n  ...\n}) -&gt;\n  loglikelihood\n\ntibble(p = seq(0.01, 0.99, 0.01),\n       loglikelihood,\n       likelihood) |&gt; \n  pivot_longer(names_to = \"type\", values_to = \"value\", loglikelihood:likelihood) |&gt; \n  ggplot(aes(p, value))+\n  geom_line()+\n  geom_vline(xintercept = 0.33, linetype = 2)+\n  facet_wrap(~type, scales = \"free_y\", nrow = 2)+\n  scale_x_continuous(breaks = c(0:5*0.25, 0.33))",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Метод максимального правдоподобия</span>"
    ]
  },
  {
    "objectID": "02-likelihood.html#послесловие",
    "href": "02-likelihood.html#послесловие",
    "title": "2  Метод максимального правдоподобия",
    "section": "2.6 Послесловие",
    "text": "2.6 Послесловие\n\nНе стоит путать метод максимального правдоподобия (MLE) c градиентным спуском\nМетод максимального правдоподобия — не единственный способ оценить параметр, смотрите, например, Maximum Spacing Estimation (Cheng and Amin 1983; Ranneby 1984; Anatolyev and Kosenok 2005), см. msedist() из пакета fitdistrplus.\n\n\n\n\n\nAnatolyev, Stanislav, and Grigory Kosenok. 2005. “An Alternative to Maximum Likelihood Based on Spacings.” Econometric Theory 21 (2): 472–76.\n\n\nCheng, R. C. H., and N. A. K. Amin. 1983. “Estimating Parameters in Continuous Univariate Distributions with a Shifted Origin.” Journal of the Royal Statistical Society: Series B (Methodological) 45 (3): 394–403.\n\n\nCoretta, Stefano. 2016. “Vowel Duration and Aspiration Effects in Icelandic.” University of York.\n\n\nHillenbrand, James, Laura A Getty, Michael J Clark, and Kimberlee Wheeler. 1995. “Acoustic Characteristics of American English Vowels.” The Journal of the Acoustical Society of America 97 (5): 3099–3111.\n\n\nRanneby, Bo. 1984. “The Maximum Spacing Method. An Estimation Method Related to the Maximum Likelihood Method.” Scandinavian Journal of Statistics, 93–112.\n\n\nRosenbach, Anette. 2003. “Aspects of Iconicity and Economy in the Choice Between the s-Genitive and the of-Genitive in English.” In Determinants of Grammatical Variation in English, edited by Günter Rohdenburg and Britta Mondorf. Berlin, New York: Mouton de Gruyter.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Метод максимального правдоподобия</span>"
    ]
  },
  {
    "objectID": "02-likelihood.html#footnotes",
    "href": "02-likelihood.html#footnotes",
    "title": "2  Метод максимального правдоподобия",
    "section": "",
    "text": "Это просто свойство логарифмов: log(5*5) = log(5)+log(5)↩︎",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Метод максимального правдоподобия</span>"
    ]
  },
  {
    "objectID": "03-mixture-models.html",
    "href": "03-mixture-models.html",
    "title": "3  Модели смеси распределений",
    "section": "",
    "text": "3.1 Cмеси распределений\nНе все переменные выглядят так же красиво, как распределения из учебников статистики. Для примера возьмем датасет, который содержит спамерские и обычные смс-сообщения, выложенный UCI Machine Learning на kaggle. Посчитаем количество символов в сообщениях:\nspam_sms &lt;- read_csv(\"https://raw.githubusercontent.com/agricolamz/2024_HSE_b_da4l/main/data/spam_sms.csv\")\n\nglimpse(spam_sms)\n\nRows: 5,572\nColumns: 2\n$ type    &lt;chr&gt; \"ham\", \"ham\", \"spam\", \"ham\", \"ham\", \"spam\", \"ham\", \"ham\", \"spa…\n$ message &lt;chr&gt; \"Go until jurong point, crazy.. Available only in bugis n grea…\n\nspam_sms |&gt; \n  mutate(n_char = nchar(message)) -&gt;\n  spam_sms\n  \nglimpse(spam_sms)\n\nRows: 5,572\nColumns: 3\n$ type    &lt;chr&gt; \"ham\", \"ham\", \"spam\", \"ham\", \"ham\", \"spam\", \"ham\", \"ham\", \"spa…\n$ message &lt;chr&gt; \"Go until jurong point, crazy.. Available only in bugis n grea…\n$ n_char  &lt;int&gt; 111, 29, 155, 49, 61, 147, 77, 160, 157, 154, 109, 136, 155, 1…\n\nspam_sms |&gt; \n  ggplot(aes(n_char))+\n  geom_histogram(fill = \"gray90\")+\n  labs(caption = \"данные из kaggle.com/uciml/sms-spam-collection-dataset\",\n       x = \"количество символов\",\n       y = \"количество сообщений\")\nМы видим два явных горба и, как можно догадаться, это связано с тем, что спамерские сообщения в среднем длиннее и сосредоточены вокруг ограничения смс в 160 символов:\nspam_sms |&gt; \n  ggplot(aes(n_char))+\n  geom_histogram(fill = \"gray70\", aes(y = after_stat(density)))+\n  geom_density(aes(fill = type), alpha = 0.3)+\n  labs(caption = \"данные из kaggle.com/uciml/sms-spam-collection-dataset\",\n       x = \"количество символов\",\n       y = \"значение функции плотности\")+\n  geom_vline(xintercept = 160, linetype = 2, size = 0.3)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Модели смеси распределений</span>"
    ]
  },
  {
    "objectID": "03-mixture-models.html#модели-смеси-распределений",
    "href": "03-mixture-models.html#модели-смеси-распределений",
    "title": "3  Модели смеси распределений",
    "section": "3.2 Модели смеси распределений",
    "text": "3.2 Модели смеси распределений\nТакого рода данные можно описать при помощи модели смеси разных распределений. Мы сейчас опишем нормальными распределениями и будем использовать пакет mixtools (для смесей нормальных распределений лучше использовать пакет mclust), но, ясно, что семейство распределений можно было бы подобрать и получше.\n\nlibrary(mixtools)\n\nset.seed(42)\nspam_length_est &lt;- normalmixEM(spam_sms$n_char)\n\nnumber of iterations= 73 \n\nsummary(spam_length_est)\n\nsummary of normalmixEM object:\n          comp 1     comp 2\nlambda  0.439334   0.560666\nmu     37.858905 114.070490\nsigma  13.398985  60.921536\nloglik at estimate:  -29421.36 \n\n\nКласс, получаемый в результате работы функции normalmixEM() имеет встроеный график:\n\nplot(spam_length_est, density = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nОднако, если хочется больше контроля над получаемым разультатом, я бы предложил использовать ggplot():\n\nnew_dnorm &lt;- function(x, mu, sigma, lambda){\n  dnorm(x, mu, sigma)*lambda\n}\n\nspam_sms |&gt; \n  ggplot(aes(n_char))+\n  geom_histogram(aes(y = after_stat(density)), fill = \"gray90\")+\n  stat_function(fun = new_dnorm, \n                args = c(mu = spam_length_est$mu[1],\n                                          sigma = spam_length_est$sigma[1],\n                                          lambda = spam_length_est$lambda[1]),\n                color = \"#F8766D\")+\n  stat_function(fun = new_dnorm, \n                args = c(mu = spam_length_est$mu[2],\n                                          sigma = spam_length_est$sigma[2],\n                                          lambda = spam_length_est$lambda[2]),\n                color = \"#00BFC4\")+\n  labs(caption = \"данные из kaggle.com/uciml/sms-spam-collection-dataset\",\n       x = \"количество символов\",\n       y = \"значение функции плотности\")+\n  geom_vline(xintercept = 160, linetype = 2, size = 0.3)\n\n\n\n\n\n\n\n\nТаким образом мы получили классификатор\n\nfirst &lt;- new_dnorm(seq(1, 750, by = 1), \n                   mu = spam_length_est$mu[1],\n                   sigma = spam_length_est$sigma[1],\n                   lambda = spam_length_est$lambda[1])\nsecond &lt;- new_dnorm(seq(1, 750, by = 1), \n                    mu = spam_length_est$mu[2],\n                    sigma = spam_length_est$sigma[2],\n                    lambda = spam_length_est$lambda[2])\nwhich(first &gt; second)\n\n [1]  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30\n[26] 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55\n[51] 56 57 58 59 60 61 62\n\n\nЕсли в смс-сообщении больше 62 символов, то согласно нашей модели, вероятнее всего это спам.\n\nspam_sms |&gt; \n  mutate(model_predict = ifelse(n_char &gt; 63, \"predicted_spam\", \"predicted_ham\")) |&gt; \n  count(model_predict, type) |&gt; \n  pivot_wider(names_from = type, values_from = n)\n\n\n  \n\n\n\nРезультат не идеальный, но лучше чем помечать как спам каждое 13 сообщение \\(747/(4825+747)\\).\n\n\n\n\n\n\nВ работе (Coretta 2016) собраны данные длительности исландских гласных. Отфильтруйте данные, оставив наблюдения гласного [a] (переменная vowel), произнесенные носителем tt01 (переменная speaker) и постройте следующие графики, моделируя длительность гласного (переменная vowel.dur) смесью трех нормальных распределений. Как вам кажется, насколько хорошо модель смеси справилась с заданием?\n\n\n\n\n\nnumber of iterations= 114",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Модели смеси распределений</span>"
    ]
  },
  {
    "objectID": "03-mixture-models.html#несколько-замечаний",
    "href": "03-mixture-models.html#несколько-замечаний",
    "title": "3  Модели смеси распределений",
    "section": "3.3 Несколько замечаний",
    "text": "3.3 Несколько замечаний\n\nВ наших примерах нам была доступна информация о классах (spam/ham, coronal/labial/velar), однако модель смесей распределений как раз имеет смысл применять, когда такой информации нет.\nВ смеси распределений может быть любое количество распределений.\nМодели смеси распределений не ограничены только нормальным распределением, алгоритм можно использовать и для других распределений.\nЧаще всего в моделях смеси распределений используются распределения одного семейства, однако можно себе представить и комбинации посложнее.\nМодели смеси распределений (mixture models) не стоит путать со смешанными моделями (mixed effects models).\n\n\n\n\n\nCoretta, Stefano. 2016. “Vowel Duration and Aspiration Effects in Icelandic.” University of York.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Модели смеси распределений</span>"
    ]
  },
  {
    "objectID": "04-intro-to-Bayes.html",
    "href": "04-intro-to-Bayes.html",
    "title": "4  Байесовский статистический вывод",
    "section": "",
    "text": "4.1 Нотация\nВ байесовском подходе статистический вывод описывается формулой Байеса\n\\[P(θ|Data) = \\frac{P(Data|θ)\\times P(θ)}{P(Data)}\\]\nВ литературе можно еще встретить такую запись:\n\\[P(θ|Data) \\propto P(Data|θ)\\times P(θ)\\]\nНа прошлых занятиях мы говорили, что функция правдоподобия не обязана интегрироваться до 1, тогда почему, назвав часть формулы Байеса \\(P(Data|θ)\\) функцией правдоподобия, мы оставляем нотацию, будто это функция вероятностей? Потому что это условная вероятность, она не обязана интегрироваться до 1.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Байесовский статистический вывод</span>"
    ]
  },
  {
    "objectID": "04-intro-to-Bayes.html#нотация",
    "href": "04-intro-to-Bayes.html#нотация",
    "title": "4  Байесовский статистический вывод",
    "section": "",
    "text": "\\(P(θ|Data)\\) — апостериорная вероятность (posterior)\n\\(P(Data|θ)\\) — функция правдоподобия (likelihood)\n\\(P(θ)\\) — априорная вероятность (prior)\n\\(P(Data)\\) — нормализующий делитель",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Байесовский статистический вывод</span>"
    ]
  },
  {
    "objectID": "04-intro-to-Bayes.html#категориальный-пример",
    "href": "04-intro-to-Bayes.html#категориальный-пример",
    "title": "4  Байесовский статистический вывод",
    "section": "4.2 Категориальный пример",
    "text": "4.2 Категориальный пример\nДля примера я взял датасет, который содержит спамерские и обычные смс-сообщения, выложенный UCI Machine Learning на kaggle и при помощи пакета udpipe токенизировал и определил часть речи:\n\nsms_pos &lt;- read_csv(\"https://raw.githubusercontent.com/agricolamz/2024_HSE_b_da4l/main/data/spam_sms_pos.csv\")\nglimpse(sms_pos)\n\nRows: 34\nColumns: 3\n$ type &lt;chr&gt; \"ham\", \"ham\", \"ham\", \"ham\", \"ham\", \"ham\", \"ham\", \"ham\", \"ham\", \"h…\n$ upos &lt;chr&gt; \"ADJ\", \"ADP\", \"ADV\", \"AUX\", \"CCONJ\", \"DET\", \"INTJ\", \"NOUN\", \"NUM\"…\n$ n    &lt;dbl&gt; 4329, 5004, 5832, 5707, 1607, 3493, 1676, 12842, 1293, 2424, 1144…\n\nsms_pos |&gt; \n  group_by(type) |&gt; \n  mutate(ratio = n/sum(n),\n         upos = fct_reorder(upos, n, mean, .desc = TRUE)) |&gt;\n  ggplot(aes(type, ratio))+\n  geom_col()+\n  geom_label(aes(label = round(ratio, 3)), position = position_stack(vjust = 0.5))+\n  facet_wrap(~upos, scales = \"free_y\")\n\n\n\n\n\n\n\n\nДавайте полученные доли считать нашей моделью: сумма всех чисел внутри каждого типа (ham/spam) дает в сумме 1. Мы получили новое сообщение:\n\nCall FREEPHONE 0800 542 0825 now!\n\nМодель udpipe разобрала его следующим образом:\n\nVERB NUM NUM NUM NUM ADV PUNCT\n\nПонятно, что это – спам, но мы попытаемся применить байесовский статистический вывод, чтобы определить тип сообщения. Предположим, что машина считает обе гипотезы равновероятными, т. е. ее априорное распределение гипотез равно 0.5 каждая. На минуту представим, что машина анализирует текст пословно. Первое слово типа VERB. Функции правдоподобия равны 0.135 и 0.096 для сообщений типа ham и spam соответственно. Применим байесовский апдейт:\n\ntibble(model = c(\"ham\", \"spam\"),\n       prior = 0.5,\n       likelihood = c(0.135, 0.096),\n       product = prior*likelihood,\n       posterior = product/sum(product))\n\n\n  \n\n\n\nВот мы и сделали байесовский апдейт. Теперь апостериорное распределение, которое мы получили на предыдущем шаге, мы можем использовать в новом апдейте. Следующее слово в сообщении типа NUM.\n\ntibble(model = c(\"ham\", \"spam\"),\n       prior_2 = c(0.584, 0.416),\n       likelihood_2 = c(0.016, 0.117),\n       product_2 = prior_2*likelihood_2,\n       posterior_2 = product_2/sum(product_2))\n\n\n  \n\n\n\nУже на второй итерации, наша модель почти уверена, что это сообщение spam. На третьей итерации уверенность только растет:\n\ntibble(model = c(\"ham\", \"spam\"),\n       prior_3 = c(0.161, 0.839),\n       likelihood_3 = c(0.016, 0.117),\n       product_3 = prior_3*likelihood_3,\n       posterior_3 = product_3/sum(product_3))\n\n\n  \n\n\n\n\n\n\n\n\n\nНа основе первых трех слов посчитайте посчитайте вероятность гипотезы, что перед нами спамерское сообщение, если предположить, что каждое пятое сообщение – спам. Ответ округлите до трех знаков после запятой.\n\n\n\n\n\n\n\n\n\n\n\n\n\nИз формулы Байеса следует, что не обязательно каждый раз делить на нормализующий делитель, это можно сделать единожды.\n\ntibble(model = c(\"ham\", \"spam\"),\n       prior = 0.5,\n       likelihood = c(0.135, 0.096),\n       likelihood_2 = c(0.016, 0.117),\n       product = prior*likelihood*likelihood_2*likelihood_2,\n       posterior = product/sum(product))\n\n\n  \n\n\n\nИз приведенных рассуждений также следует, что все равно в каком порядке мы производим байесовский апдейт: мы могли сначала умножить на значение правдоподобия для категории NUM и лишь в конце на значение правдоподобия VERB.\nТакже стоит отметить, что если данных много, то через какое-то время становится все равно, какое у нас было априорное распределение. Даже в нашем примере, в котором мы проанализировали первые три слова сообщения, модель, прогнозирующая, что сообщение спамерское, выиграет, даже если, согласно априорному распределению, спамерским является каждое 20 сообщение:\n\ntibble(model = c(\"ham\", \"spam\"),\n       prior = c(0.95, 0.05),\n       likelihood = c(0.135, 0.096),\n       likelihood_2 = c(0.016, 0.117),\n       product = prior*likelihood*likelihood_2*likelihood_2,\n       posterior = product/sum(product))\n\n\n  \n\n\n\nСамым главным отличием байесовского статистического вывода от фриквентистского, является то, что мы в результате получаем вероятность каждой из моделей. Это очень значительно отличается от фриквентистской практики нулевых гипотез и p-value, в соответствии с которыми мы можем лишь отвергнуть или не отвергнуть нулевую гипотезу.\n\n\n\n\n\n\nВашего друга похитили а на почту отправили датасет, в котором записаны данные о погоде из пяти городов. Ваш телефон зазвонил, и друг сказал, что не знает куда его похитили, но за окном легкий дождь (Rain). А в какой-то из следующих дней — сильный дождь (Rain, Thunderstorm). Исходя из явно неверного предположения, что погодные условия каждый день не зависят друг от друга, сделайте байесовский апдейт и предположите, в какой город вероятнее всего похитили друга.\n\n\n\n\n\n\n\nAuckland\nBeijing\nChicago\nMumbai\nSan Diego\n\n\n\n\n\n\n\n\n\n\n\n\n\nУкажите получившуюся вероятность. Выполняя задание, округлите все вероятности и значения правдоподобия до 3 знаков после запятой.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Байесовский статистический вывод</span>"
    ]
  },
  {
    "objectID": "04-intro-to-Bayes.html#разница-между-фриквентиским-и-байесовским-подходами",
    "href": "04-intro-to-Bayes.html#разница-между-фриквентиским-и-байесовским-подходами",
    "title": "4  Байесовский статистический вывод",
    "section": "4.3 Разница между фриквентиским и байесовским подходами",
    "text": "4.3 Разница между фриквентиским и байесовским подходами\n\nКартинка из одной из моих любимых книг по статистике (Efron and Hastie 2016: 34).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Байесовский статистический вывод</span>"
    ]
  },
  {
    "objectID": "04-intro-to-Bayes.html#биномиальные-данные",
    "href": "04-intro-to-Bayes.html#биномиальные-данные",
    "title": "4  Байесовский статистический вывод",
    "section": "4.4 Биномиальные данные",
    "text": "4.4 Биномиальные данные\nБиномиальные данные возникают, когда нас интересует доля успехов в какой-то серии эксперементов Бернулли.\n\n4.4.1 Биномиальное распределение\nБиномиальное распределение — распределение количества успехов эксперементов Бернулли из n попыток с вероятностью успеха p.\n\\[P(k | n, p) = \\frac{n!}{k!(n-k)!} \\times p^k \\times (1-p)^{n-k} =  {n \\choose k} \\times p^k \\times (1-p)^{n-k}\\] \\[ 0 \\leq p \\leq 1; n, k &gt; 0\\]\n\ntibble(x = 0:50,\n           density = dbinom(x = x, size = 50, prob = 0.16)) |&gt; \n  ggplot(aes(x, density))+\n  geom_point()+\n  geom_line()+\n  labs(title = \"Биномиальное распределение p = 0.16, n = 50\")\n\n\n\n\n\n\n\n\n\n\n4.4.2 Бета распределение\n\\[P(x; α, β) = \\frac{x^{α-1}\\times (1-x)^{β-1}}{B(α, β)}; 0 \\leq x \\leq 1; α, β &gt; 0\\]\nБета функция:\n\\[Β(α, β) = \\frac{Γ(α)\\times Γ(β)}{Γ(α+β)} = \\frac{(α-1)!(β-1)!}{(α+β-1)!} \\]\n\ntibble(x = seq(0, 1, length.out = 100),\n           density = dbeta(x = x, shape1 = 8, shape2 = 42)) |&gt; \n  ggplot(aes(x, density))+\n  geom_point()+\n  geom_line()+\n  labs(title = \"Бета распределение α = 8, β = 42\")\n\n\n\n\n\n\n\n\nМожно поиграть с разными параметрами:\n\nshiny::runGitHub(\"agricolamz/beta_distribution_shiny\") \n\n\\[\\mu = \\frac{\\alpha}{\\alpha+\\beta}\\]\n\\[\\sigma^2 = \\frac{\\alpha\\times\\beta}{(\\alpha+\\beta)^2\\times(\\alpha+\\beta+1)}\\]\n\n\n4.4.3 Байесовский апдейт биномиальных данных\n\\[Beta_{post}(\\alpha_{post}, \\beta_{post}) = Beta(\\alpha_{prior}+\\alpha_{data}, \\beta_{prior}+\\beta_{data}),\\] где \\(Beta\\) — это бета распределение\n\nshiny::runGitHub(\"agricolamz/bayes_for_binomial_app\") \n\n\n\n\n\n\n\nНемного упрощая данные из статьи (Rosenbach 2003: 394), можно сказать что носители британского английского предпочитают s-генитив (90%) of-генитиву (10%). Проведите байесовский апдейт, если Вы наблюдаете в интервью британского актера из 120 контекстов 92 s-генитивов. Априорное распределение берите соразмерное данным. Ответ округлите до трёх или менее знаков после запятой.\n\n\n\n\n\n\n\n\n\nПараметр альфа:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nПараметр бета:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.4.4 Байесовский апдейт биномиальных данных: несколько моделей\n\ntibble(x = rep(seq(0, 1, length.out = 100), 6),\n           density = c(dbeta(unique(x), shape1 = 8, shape2 = 42),\n                       dbeta(unique(x), shape1 = 16, shape2 = 34),\n                       dbeta(unique(x), shape1 = 24, shape2 = 26),\n                       dbeta(unique(x), shape1 = 8+4, shape2 = 42+16),\n                       dbeta(unique(x), shape1 = 16+4, shape2 = 34+16),\n                       dbeta(unique(x), shape1 = 24+4, shape2 = 26+16)),\n           type = rep(c(\"prior\", \"prior\", \"prior\", \"posterior\", \"posterior\", \"posterior\"), each = 100),\n           dataset = rep(c(\"prior: 8, 42\", \"prior: 16, 34\", \"prior: 24, 26\",\n                           \"prior: 8, 42\", \"prior: 16, 34\", \"prior: 24, 26\"), each = 100)) |&gt; \n  ggplot(aes(x, density, color = type))+\n  geom_line()+\n  facet_wrap(~dataset)+\n  labs(title = \"data = 4, 16\")\n\n\n\n\n\n\n\n\n\n\n4.4.5 Что почитать?\nЕсли остались неясности, то можно посмотреть 2-ую главу (Robinson 2017).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Байесовский статистический вывод</span>"
    ]
  },
  {
    "objectID": "04-intro-to-Bayes.html#байесовский-апдейт-нормального-распределения",
    "href": "04-intro-to-Bayes.html#байесовский-апдейт-нормального-распределения",
    "title": "4  Байесовский статистический вывод",
    "section": "4.5 Байесовский апдейт нормального распределения",
    "text": "4.5 Байесовский апдейт нормального распределения\nВстроенный датасет ChickWeight содержит вес цыплят (weight) в зависимости от типа диеты (Diet). Мы будем анализировать 20-дневных птенцов.\n\nChickWeight |&gt; \n  filter(Time == 20) -&gt;\n  chicks\n\nchicks |&gt; \n  ggplot(aes(weight))+\n  geom_density()\n\n\n\n\n\n\n\n\nНачнем с апостериорных параметров для наблюдений \\(x_1, ... x_n\\) со средним \\(\\mu_{data}\\) известной дисперсией \\(\\sigma_{known}^2\\)\n\n4.5.1 Байесовский апдейт нормального распределения: выбор из нескольких моделей\nМы можем рассматривать эту задачу как выбор между несколькими моделями с разными средними:\n\ntibble(x = rep(1:400, 6),\n           density = c(dnorm(unique(x), mean = 125, sd = 70),\n                       dnorm(unique(x), mean = 150, sd = 70),\n                       dnorm(unique(x), mean = 175, sd = 70),\n                       dnorm(unique(x), mean = 200, sd = 70),\n                       dnorm(unique(x), mean = 225, sd = 70),\n                       dnorm(unique(x), mean = 250, sd = 70)),\n           dataset = rep(1:6, each = 400)) |&gt; \n  ggplot(aes(x, density, color = factor(dataset)))+\n  geom_line()\n\n\n\n\n\n\n\n\nДальше мы можем точно так же апдейтить, как мы делали раньше:\n\ntibble(mu = seq(125, 250, by = 25),\n           prior = 1/6,\n           likelihood = c(prod(dnorm(chicks$weight, mean = 125, sd = 70)),\n                          prod(dnorm(chicks$weight, mean = 150, sd = 70)),\n                          prod(dnorm(chicks$weight, mean = 175, sd = 70)),\n                          prod(dnorm(chicks$weight, mean = 200, sd = 70)),\n                          prod(dnorm(chicks$weight, mean = 225, sd = 70)),\n                          prod(dnorm(chicks$weight, mean = 250, sd = 70))),\n           product = prior*likelihood,\n           posterior = product/sum(product)) -&gt;\n  results\nresults\n\n\n  \n\n\nresults |&gt; \n  select(mu, prior, posterior) |&gt; \n  pivot_longer(names_to = \"type\", values_to = \"probability\", prior:posterior) |&gt; \n  ggplot(aes(mu, probability, color = type))+\n  geom_point()+\n  labs(title = \"изменение вероятностей для каждой из моделей\",\n       x = \"μ\")\n\n\n\n\n\n\n\n\n\n\n4.5.2 Байесовский апдейт нормального распределения: непрерывный вариант\nВо первых, нам понадобится некоторая мера, которая называется точность (precision):\n\\[\\tau = \\frac{1}{\\sigma^2}\\]\n\\[\\tau_{post} = \\tau_{prior} + \\tau_{data} \\Rightarrow \\sigma^2_{post} = \\frac{1}{\\tau_{post}}\\]\n\\[\\mu_{post} = \\frac{\\mu_{prior} \\times \\tau_{prior} + \\mu_{data} \\times \\tau_{data}}{\\tau_{post}}\\]\nТак что если нашим априорным распределением мы назовем нормальное распределение со средним около 180 и стандартным отклонением 90, то процесс байесовского апдейта будет выглядеть вот так:\n\nsd_prior &lt;- 90 \nsd_data &lt;- sd(chicks$weight)\nsd_post &lt;- 1/sqrt(1/sd_prior^2 + 1/sd_data^2)\nmean_prior &lt;- 180\nmean_data &lt;- mean(chicks$weight)\nmean_post &lt;- weighted.mean(c(mean_prior, mean_data), c(1/sd_prior^2, 1/sd_data^2))\n\nchicks |&gt; \n  ggplot(aes(weight)) +\n  geom_histogram(aes(y = after_stat(density)))+\n  stat_function(fun = dnorm, args = list(mean_prior,  sd_prior), color = \"lightblue\")+\n  stat_function(fun = dnorm, args = list(mean_post,  sd_post), color = \"red\")\n\n\n\n\n\n\n\n\n\nshiny::runGitHub(\"agricolamz/bayes_for_normal_app\") \n\n\n\n\n\n\n\nВ работе (Coretta 2016) собраны данные длительности исландских гласных. Отфильтруйте данные, произнесенные носителем tt01 (переменная speaker), произведите байесовский апдейт данных, моделируя длительность гласных (переменная vowel.dur) нормальным распределением и постройте график. В качестве априорного распределения используйте нормальное распределение со средним 87 и стандартным отклонением 25.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.5.3 Что почитать?\n\nMurphy K. P. (2007) Conjugate Bayesian analysis of the Gaussian distribution\nJordan M. I. (2010) The Conjugate Prior for the Normal Distribution\nраздел 2.5 в Gelman A. et. al (2014) Bayesian Data Analysis",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Байесовский статистический вывод</span>"
    ]
  },
  {
    "objectID": "04-intro-to-Bayes.html#другие-распределения",
    "href": "04-intro-to-Bayes.html#другие-распределения",
    "title": "4  Байесовский статистический вывод",
    "section": "4.6 Другие распределения",
    "text": "4.6 Другие распределения\nМы обсудили биномиальные и нормальное распределенные данные. Так случилось, что для них есть короткий путь сделать байесовский апдейт, не применяя формулы Байеса. И нам так повезло, что связки априорного/апосториорного распределений и функции правдоподобия такие простые:\n\nаприорного/апосториорного распределены как бета распределение, значит функция правдоподобия – биномиальное распределение\nесли мы моделируем данные при помощи нормального распределения, то все три распределения (априорное, функция правдопдобия и апосториорное) – нормальные.\n\nТакие отношения между распределениями называют сопряженными (conjugate). В результате для разных семейств функции правдоподобия существует список соответствующих сопряженных априорных распределений (conjugate prior), который можно найти, например, здесь.\nВ большинстве случаев используется (а на самом деле почти всегда) Марковские цепи Монте-Карло (MCMC).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Байесовский статистический вывод</span>"
    ]
  },
  {
    "objectID": "04-intro-to-Bayes.html#вопросы-к-апостериорному-распределению",
    "href": "04-intro-to-Bayes.html#вопросы-к-апостериорному-распределению",
    "title": "4  Байесовский статистический вывод",
    "section": "4.7 Вопросы к апостериорному распределению",
    "text": "4.7 Вопросы к апостериорному распределению\n\nA frequentist uses impeccable logic to answer the wrong question, while a Bayesian answers the right question by making assumptions that nobody can fully believe in. (P. G. Hammer)\n\n\nпопытка оценить параметр θ и/или какой-нибудь интервал, в котором он лежит.\n\nсреднее апостериорного распределения (mean of the posterior estimation, MAP)\nмаксимум апостериорного распределения (maximum a posteriori estimation, MAP)\nбайесовский доверительный интервал\n\nответить на вопросы вроде\n\nкакова вероятность, что значение θ больше некоторого значения \\(x\\)?\nкакова вероятность, что значение θ лежит в интервале \\([x; y]\\)?\nи т. п.\n\nВыборки из апостериорного распределения (Posterior simulation):\n\nсимулируйте большую выборку из апостериорного распределения;\nиспользуйте полученную выборку для статистического вывода.\n\n\nДопустим, мы получили апостериорное бета распределение с параметрами 20 и 70. Какова вероятность наблюдать значения больше 0.3?\n\nposterior_simulation &lt;- rbeta(n = 10000, shape1 = 20, shape2 = 70)\nsum(posterior_simulation &gt; 0.3)/10000\n\n[1] 0.0436\n\n\nИ это не p-value! Это настоящие вероятности!\n\n\n\n\nCoretta, Stefano. 2016. “Vowel Duration and Aspiration Effects in Icelandic.” University of York.\n\n\nEfron, Bradley, and Trevor Hastie. 2016. Computer Age Statistical Inference. Vol. 5. Cambridge University Press.\n\n\nRobinson, D. 2017. Introduction to Empirical Bayes: Examples from Baseball Statistics. ASIN: B06WP26J8Q.\n\n\nRosenbach, Anette. 2003. “Aspects of Iconicity and Economy in the Choice Between the s-Genitive and the of-Genitive in English.” In Determinants of Grammatical Variation in English, edited by Günter Rohdenburg and Britta Mondorf. Berlin, New York: Mouton de Gruyter.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Байесовский статистический вывод</span>"
    ]
  },
  {
    "objectID": "05-confidence-intervals.html",
    "href": "05-confidence-intervals.html",
    "title": "5  Байесовский доверительный интервал",
    "section": "",
    "text": "5.1 Фреквентисткий доверительный интервал\nРассмотрим простенькую задачу, которую мы видели раньше:\nФреквентистский доверительный интервал (по-английски confidence interval) основан на правиле трех сигм нормального распределения:\nz-score:\nДоверительный интервал:\n\\[\\bar{x} \\pm z \\times \\frac{\\sigma}{\\sqrt{n}}\\text{, где } z \\text{ — это z-оценка } 1 - \\frac{\\alpha}{2} \\text{ часть данных}\\]\nРаспространение этой логики на биномиальные данные называется интервал Вальда:\n\\[\\bar{x} = \\theta; \\sigma = \\sqrt{\\frac{\\theta\\times(1-\\theta)}{n}}\\]\nТогда интервал Вальда:\n\\[\\theta \\pm  z\\times\\sqrt{\\frac{\\theta\\times(1-\\theta)} {n}}\\]\nЕсть только одна проблема: работает он плохо. Его аналоги перечислены в других работ:\nlow_ci &lt;- binom.test(x = 108+92, n = 108+92+12+28)$conf.int[1]\nup_ci &lt;-  binom.test(x = 108+92, n = 108+92+12+28)$conf.int[2]\n\ntibble(x = seq(0, 1, by = 0.001),\n       y = dbeta(x, 108+92, 12+28)) |&gt;\n  ggplot(aes(x, y))+\n  geom_line()+\n  annotate(geom = \"errorbar\", y = 0, xmin = low_ci, xmax = up_ci, color = \"red\")+\n  labs(title = \"Апостериорное распределение\",\n       subtitle = \"красным фреквентисткий 95% доверительный интервал\",\n       x = \"\", y = \"\")\nВ базовом пакете функция binom.test() не позволяет выбирать тип доверительного интервала. ci.method = \"Clopper-Pearson\" возможна, если включить библиотеку mosaic.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Байесовский доверительный интервал</span>"
    ]
  },
  {
    "objectID": "05-confidence-intervals.html#фреквентисткий-доверительный-интервал",
    "href": "05-confidence-intervals.html#фреквентисткий-доверительный-интервал",
    "title": "5  Байесовский доверительный интервал",
    "section": "",
    "text": "95% данных находится в 1.96 стандартных отклонений\n99% данных находится в 2.58 стандартных отклонений\n\n\n\nпредположим, что данные генеральной совокупности нормально распределены\nтогда доверительные интервалы выборок взятых из генеральной совокупности будут покрывать среднее генеральной совокупности\n\n\n\n\n\n\n\n\nassymptotic method with continuity correction\nWilson score\nWilson Score method with continuity correction\nJeffreys interval\nClopper–Pearson interval (default in R binom.test())\nAgresti–Coull interval\n… см. пакет binom",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Байесовский доверительный интервал</span>"
    ]
  },
  {
    "objectID": "05-confidence-intervals.html#байесовский-доверительный-интервал",
    "href": "05-confidence-intervals.html#байесовский-доверительный-интервал",
    "title": "5  Байесовский доверительный интервал",
    "section": "5.2 Байесовский доверительный интервал",
    "text": "5.2 Байесовский доверительный интервал\nБайесовский доверительный \\((100-k)\\)-% интервал (по-английски credible interval) — это интервал \\([\\frac{k}{2}, 1-\\frac{k}{2}]\\) от апостериорного распределения.\n\nlow_ci &lt;- binom.test(x = 108+92, n = 108+92+12+28)$conf.int[1]\nup_ci &lt;-  binom.test(x = 108+92, n = 108+92+12+28)$conf.int[2]\n\ncred_int_l &lt;- qbeta(0.025, 108+92, 12+28)\ncred_int_h &lt;- qbeta(0.975, 108+92, 12+28)\n\ntibble(x = seq(0, 1, by = 0.001),\n       y = dbeta(x, 108+92, 12+28)) |&gt;\n  ggplot(aes(x, y))+\n  geom_line()+\n  annotate(geom = \"errorbar\", y = 0, xmin = low_ci, xmax = up_ci, color = \"red\")+\n  annotate(geom = \"errorbar\", y = -1, xmin = cred_int_l, xmax = cred_int_h, color = \"lightblue\")+\n  labs(title = \"Апостериорное распределение\",\n       subtitle = \"красным фреквентисткий 95% доверительный интервал\\nсиним байесовский 95% доверительный интервал\",\n       x = \"\", y = \"\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nВ работе (Coretta 2016) собраны данные длительности исландских гласных. Отфильтруйте данные, произнесенные носителем tt01 (переменная speaker), произведите байесовский апдейт данных, моделируя длительность гласных (переменная vowel.dur) нормальным распределением и постройте график. На графике отобразите 80% и 95% байесовский доверительный интервал (при построении интервала я использовал аргумент width = 0.001). В качестве априорного распределения используйте нормальное распределение со средним 87 и стандартным отклонением 25.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCoretta, Stefano. 2016. “Vowel Duration and Aspiration Effects in Icelandic.” University of York.\n\n\nRosenbach, Anette. 2003. “Aspects of Iconicity and Economy in the Choice Between the s-Genitive and the of-Genitive in English.” In Determinants of Grammatical Variation in English, edited by Günter Rohdenburg and Britta Mondorf. Berlin, New York: Mouton de Gruyter.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Байесовский доверительный интервал</span>"
    ]
  },
  {
    "objectID": "06-Bayes-factor.html",
    "href": "06-Bayes-factor.html",
    "title": "6  Коэффициент Байеса",
    "section": "",
    "text": "6.1 Формула Байеса опять\n\\[P(\\theta|Data) = \\frac{P(Data|\\theta) \\times  P(\\theta) }{P(Data)}\\]\nРассмотрим какой-то простой случай, который мы уже видели много раз.\nЕсли мы не будем следовать простой дорожкой, которую мы обсуждали несколько разделов назад, а будем все делать согласно формуле Байеса, то получатся следующие компоненты:\ntibble(x = seq(0, 1, 0.001),\n       prior = dbeta(x = x, shape1 = 120*0.9, shape2 = 120*0.1)) |&gt; \n  ggplot(aes(x, prior))+\n  geom_line(color = \"red\")\ntibble(x = seq(0, 1, 0.001),\n       likelihood = dbinom(x = 92, size = 120, prob = x)) |&gt; \n  ggplot(aes(x, likelihood))+\n  geom_line()\ntibble(x = seq(0, 1, 0.001),\n       prior = dbeta(x = x, shape1 = 120*0.9, shape2 = 120*0.1),\n       likelihood = dbinom(x = 92, size = 120, prob = x),\n       product = prior*likelihood) |&gt; \n  ggplot(aes(x, product))+\n  geom_line()\nmarginal_likelihood &lt;- integrate(function(p){\n  dbinom(92, 120, p) * dbeta(p, 120*0.9, 120*0.1)}, \n  lower = 0, \n  upper = 1)\nmarginal_likelihood\n\n0.0009531395 with absolute error &lt; 0.000044\ntibble(x = seq(0, 1, 0.001),\n       prior = dbeta(x = x, shape1 = 120*0.9, shape2 = 120*0.1),\n       likelihood = dbinom(x = 92, size = 120, prob = x),\n       product = prior*likelihood,\n       posterior = product/marginal_likelihood[[1]]) |&gt; \n  ggplot(aes(x, posterior))+\n  geom_line(color = \"darkgreen\")+\n  geom_line(aes(y = prior), color = \"red\")\n… которое мы умеем доставать и быстрее:\ntibble(x = seq(0, 1, 0.001),\n       prior = dbeta(x = x, shape1 = 120*0.9, shape2 = 120*0.1),\n       likelihood = dbinom(x = 92, size = 120, prob = x),\n       product = prior*likelihood,\n       posterior = product/marginal_likelihood[[1]],\n       posterior_2 = dbeta(x = x, shape1 = 120*0.9+92, shape2 = 120*0.1+120-92)) |&gt; \n  ggplot(aes(x, posterior))+\n  geom_line(color = \"darkgreen\", size = 2)+\n  geom_line(aes(y = prior), color = \"red\")+\n  geom_line(aes(y = posterior_2), linetype = 2, color = \"yellow\")\nПредставим себе, что у нас есть \\(k\\) гипотез \\(M\\). Тогда формула Байеса может выглядеть вот так:\n\\[P(M_k|Data) = \\frac{P(Data|M_k) \\times  P(M_k) }{P(Data)}\\] В данном занятии мы рассмотрим только случай двух модели, но можно рассматривать и случаи, когда моделей много. Посмотрим на соотношение апостериорных распределений двух моделей:\n\\[\\underbrace{\\frac{P(M_1 \\mid Data)}{P(M_2 \\mid Data)}}_{\\text{posterior odds}} = \\frac{\\frac{P(Data|M_1) \\times  P(M_1) }{P(Data)}}{\\frac{P(Data|M_2) \\times  P(M_2) }{P(Data)}}=\\underbrace{\\frac{P(Data \\mid M_1)}{P(Data \\mid M_2)}}_{\\text{Bayes factor}}\\times\\underbrace{\\frac{P(M_1)}{P(M_2)}}_{\\text{prior odds}}\\]\nТаким образом байесовский коэффициент это соотношение апосториорных распределений деленное на соотношение априорных распределений.\n\\[BF_{12}= \\frac{P(M_1 \\mid Data)/P(M_2 \\mid Data)}{P(M_1)/P(M_2)}=\\frac{P(M_1 \\mid Data)\\times P(M_2)}{P(M_2 \\mid Data)\\times P(M_1)}\\]\nВ результате получается, что коэффициент Байеса — это соотношение предельных правдоподобий (знаменатель теоремы Байеса):\n\\[BF_{12}= \\frac{P(Data|\\theta, M_1))}{P(Data|\\theta, M_2))}=\\frac{\\int P(Data|\\theta, M_1)\\times P(\\theta|M_1)}{\\int P(Data|\\theta, M_2)\\times P(\\theta|M_2)}\\]\nВажно заметить, что если вероятности априорных моделей равны, то байесовский коэффициент равен просто соотношению функций правдоподобия.\nНадо отметить, что не все тепло относятся к сравнению моделей байесовским коэффициентом (см. (Gelman and Rubin 1995)).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Коэффициент Байеса</span>"
    ]
  },
  {
    "objectID": "06-Bayes-factor.html#формула-байеса-опять",
    "href": "06-Bayes-factor.html#формула-байеса-опять",
    "title": "6  Коэффициент Байеса",
    "section": "",
    "text": "Немного упрощая данные из статьи (Rosenbach 2003: 394), можно сказать что носители британского английского предпочитают s-генитив (90%) of-генитиву (10%). Проведите байесовский апдейт, если Вы наблюдаете в интервью британского актера из 120 контекстов 92 s-генитивов. Априорное распределение берите соразмерное данным.\n\n\n\n\n\nаприорное распределение\n\n\n\nфункция правдоподобия\n\n\n\nих произведение (пропорционально апостериорному распределению)\n\n\n\nпредельное правдоподобие, которое позволяет сделать получившееся распределение распределением вероятностей\n\n\n\n… и в результате получается апостериорное распределение!",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Коэффициент Байеса</span>"
    ]
  },
  {
    "objectID": "06-Bayes-factor.html#категориальные-данные",
    "href": "06-Bayes-factor.html#категориальные-данные",
    "title": "6  Коэффициент Байеса",
    "section": "6.2 Категориальные данные",
    "text": "6.2 Категориальные данные\nДля примера обратимся снова к датасету, который содержит спамерские и обычные смс-сообщения, выложенному UCI Machine Learning на kaggle, и при помощи пакета udpipe токенизируем и определим часть речи:\n\nsms_pos &lt;- read_csv(\"https://raw.githubusercontent.com/agricolamz/2024_HSE_b_da4l/master/data/spam_sms_pos.csv\")\nglimpse(sms_pos)\n\nRows: 34\nColumns: 3\n$ type &lt;chr&gt; \"ham\", \"ham\", \"ham\", \"ham\", \"ham\", \"ham\", \"ham\", \"ham\", \"ham\", \"h…\n$ upos &lt;chr&gt; \"ADJ\", \"ADP\", \"ADV\", \"AUX\", \"CCONJ\", \"DET\", \"INTJ\", \"NOUN\", \"NUM\"…\n$ n    &lt;dbl&gt; 4329, 5004, 5832, 5707, 1607, 3493, 1676, 12842, 1293, 2424, 1144…\n\nsms_pos |&gt; \n  group_by(type) |&gt; \n  mutate(ratio = n/sum(n),\n         upos = fct_reorder(upos, n, mean, .desc = TRUE)) |&gt;\n  ggplot(aes(type, ratio))+\n  geom_col()+\n  geom_label(aes(label = round(ratio, 3)), position = position_stack(vjust = 0.5))+\n  facet_wrap(~upos, scales = \"free_y\")\n\n\n\n\n\n\n\n\nДавайте полученные доли считать нашей моделью: сумма всех чисел внутри каждого типа (ham/spam) дает в сумме 1. Мы получили новое сообщение:\n\nCall FREEPHONE 0800 542 0825 now!\n\nМодель udpipe разобрала его следующим образом:\n\nVERB NUM NUM NUM NUM ADV PUNCT\n\nЕсли мы считаем наши модели равновероятными:\n\nfirst_update &lt;- tibble(model = c(\"ham\", \"spam\"),\n                       prior = 0.5,\n                       likelihood = c(0.135, 0.096),\n                       product = prior*likelihood,\n                       marginal_likelihood = sum(product),\n                       posterior = product/marginal_likelihood)\nfirst_update\n\n\n  \n\n\n\nЕсли же мы примем во внимание, что наши классы не равноправны, то сможем посчитать это нашим априорным распределением для моделей.\n\nsms_pos |&gt; \n  uncount(n) |&gt; \n  count(type) |&gt; \n  mutate(ratio = n/sum(n)) -&gt;\n  class_ratio\nclass_ratio\n\n\n  \n\n\nsecond_update &lt;- tibble(model = c(\"ham\", \"spam\"),\n                        prior = class_ratio$ratio,\n                        likelihood = c(0.135, 0.096),\n                        product = prior*likelihood,\n                        marginal_likelihood = sum(product),\n                        posterior = product/marginal_likelihood)\nsecond_update\n\n\n  \n\n\n# Bayes factor\nsecond_update$marginal_likelihood[1]/first_update$marginal_likelihood[1]\n\n[1] 1.098469",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Коэффициент Байеса</span>"
    ]
  },
  {
    "objectID": "06-Bayes-factor.html#интерпретация-коэфициента-байеса",
    "href": "06-Bayes-factor.html#интерпретация-коэфициента-байеса",
    "title": "6  Коэффициент Байеса",
    "section": "6.3 Интерпретация коэфициента Байеса",
    "text": "6.3 Интерпретация коэфициента Байеса",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Коэффициент Байеса</span>"
    ]
  },
  {
    "objectID": "06-Bayes-factor.html#биномиальные-данные",
    "href": "06-Bayes-factor.html#биномиальные-данные",
    "title": "6  Коэффициент Байеса",
    "section": "6.4 Биномиальные данные",
    "text": "6.4 Биномиальные данные\nРассмотрим простенькую задачу, которую мы видели раньше:\n\n\n\n\n\n\nНемного упрощая данные из статьи (Rosenbach 2003: 394), можно сказать что носители британского английского предпочитают s-генитив (90%) of-генитиву (10%), а носители американского английского предпочитают s-генитив (85%) of-генитиву (15%). Мы наблюдаем актера, который в интервью из 120 контекстов использует в 92 случаях s-генитивы. Сравните модели при помощи байесовского коэффициента.\n\n\n\n\ntibble(x = seq(0, 1, by = 0.001),\n       y = dbeta(x, 120*0.9, 120*0.1),\n       z = dbeta(x, 120*0.85, 120*0.15)) |&gt; \n  ggplot(aes(x, y))+\n  geom_line(color = \"red\")+\n  geom_line(aes(y = z), color = \"lightblue\")+\n  geom_vline(xintercept = 92/120, linetype = 2)\n\n\n\n\n\n\n\nm1 &lt;- function(p) dbinom(92, 120, p) * dbeta(p, 120*0.9, 120*0.1)\nm2 &lt;- function(p) dbinom(92, 120, p) * dbeta(p, 120*0.85, 120*0.15)\n\nintegrate(m1, 0, 1)[[1]]/integrate(m2, 0, 1)[[1]]\n\n[1] 0.0672068\n\n\n\n\n\n\n\n\nВ работе (Coretta 2016) собраны данные длительности исландских гласных (столбец vowel.dur). Отфильтруйте данные, произнесенные носителем tt01 (переменная speaker), посчитайте байесовский коэффициент (\\(B_{12}\\)) для двух априорных моделей:\n\nнормального распределения со средним 87 и стандартным отклонением 25. (\\(m_1\\))\nнормального распределения со средним 85 и стандартным отклонением 30. (\\(m_2\\))\n\nОтвет округлите до трёх или менее знаков после запятой.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Коэффициент Байеса</span>"
    ]
  },
  {
    "objectID": "06-Bayes-factor.html#сравнение-точечных-и-интервальных-моделей-основано-на-etz18",
    "href": "06-Bayes-factor.html#сравнение-точечных-и-интервальных-моделей-основано-на-etz18",
    "title": "6  Коэффициент Байеса",
    "section": "6.5 Сравнение точечных и интервальных моделей (основано на (Etz et al. 2018))",
    "text": "6.5 Сравнение точечных и интервальных моделей (основано на (Etz et al. 2018))\nДо этого момента, когда мы говорили о сравнении биномиальных данных, мы обычно говорили о поиске и описании параметра p бета и биномиального распределений — которая в свою очередь представляет отражает нашу точечную оценку моделируемого процесса. Например, если мы пытаемся моделировать род слова (например, кофе), мы можем представить это в виде трех гипотез: слово относится к одному роду, к другому роду или существует вариативность:\n\n\n\n\n\n\n\n\n\nВ большинстве случаев нас интересует не все три варианта, а лишь два: слово четко характеризуется некоторым родом или же мы наблюдаем вариативность. Если же вдруг в реальности вы видите третий вариант — значит вы недостаточно подготовились к моделированию и строить гипотезы было рано.\n\n\n\n\n\n\n\n\n\nПредставим в ходе эксперимента мы опросили 16 носителей. В таком случае мы можем описать предсказания модели при помощи двух биномиальных распределений:\n\n\n\n\n\n\n\n\n\nНаш классификатор получился слишком строгий: либо все говорят слово в роде X, либо вариативность. Для того, чтобы допустить хоть какие-то поблажки, давайте ослабим параметр с 1 до 0.96:\n\n\n\n\n\n\n\n\n\nТогда предсказания модели будет выглядит вот так:\n\n\n\n\n\n\n\n\n\nНа всякий случай, соотношение высот столбиков — это фриквентистский вариант байесовского коэффициента, который называется тест отношения правдоподобия (likelihood ratio).\n\n\n\n\n\n\n\n\n\nОсновная проблема точечных оценок заключается в том, что они оставляют достаточно много неуверенности в промежуточных значениях. Представим, что у нас не 16 наблюдений, а 90:\n\n\n\n\n\n\n\n\n\nВ таком случае наш классификатор достаточно сильно не уверен в значениях между 65 и 75. Альтернативой являются интервальные модели:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nИли вот еще возможные комбинации:\n\n\n\n\n\n\n\n\n\n\n\n\n\nCoretta, Stefano. 2016. “Vowel Duration and Aspiration Effects in Icelandic.” University of York.\n\n\nEtz, Alexander, Julia M Haaf, Jeffrey N Rouder, and Joachim Vandekerckhove. 2018. “Bayesian Inference and Testing Any Hypothesis You Can Specify.” Advances in Methods and Practices in Psychological Science 1 (2): 281–95.\n\n\nGelman, A., and D. B. Rubin. 1995. “Avoiding Model Selection in Bayesian Social Research.” Sociological Methodology 25: 165–73.\n\n\nRosenbach, Anette. 2003. “Aspects of Iconicity and Economy in the Choice Between the s-Genitive and the of-Genitive in English.” In Determinants of Grammatical Variation in English, edited by Günter Rohdenburg and Britta Mondorf. Berlin, New York: Mouton de Gruyter.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Коэффициент Байеса</span>"
    ]
  },
  {
    "objectID": "07-Empirical-Bayes.html",
    "href": "07-Empirical-Bayes.html",
    "title": "7  Эмпирическая байесовская оценка",
    "section": "",
    "text": "library(tidyverse)\n\nМетод эмпирической байесовской оценки (Empirical Bayes estimation) — один из байесовских методов, в рамках которого:\n\nпроизводят оценку априорного распределения вероятностей на основании имеющихся данных\nиспользуют полученное априорное распределение для получение апостериорной оценки для каждого наблюдения\n\nРассмотрим пример данных из статьи (Daniel et al. 2019), в которой аннализировались интервью с людьми из деревени Михалёвская и исследовался ряд консервативных и инновативных черт в их речи.\n\nmikhalevskaja &lt;- read_csv(\"https://raw.githubusercontent.com/agricolamz/2024_HSE_b_da4l/master/data/ustya_data.csv\")\n\nRows: 359 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): speaker, gender, feature\ndbl (4): year, conservative, innovative, total\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nglimpse(mikhalevskaja)\n\nRows: 359\nColumns: 7\n$ speaker      &lt;chr&gt; \"avm1922\", \"ans1925\", \"avt1928\", \"egp1928\", \"lpp1928\", \"p…\n$ year         &lt;dbl&gt; 1922, 1925, 1928, 1928, 1928, 1928, 1930, 1933, 1935, 194…\n$ gender       &lt;chr&gt; \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"m\", \"f\", \"f\", \"f\", \"f\", \"f…\n$ conservative &lt;dbl&gt; 92, 56, 12, 33, 2, 83, 22, 22, 33, 60, 58, 11, 55, 8, 30,…\n$ innovative   &lt;dbl&gt; 60, 70, 46, 127, 23, 127, 41, 88, 85, 103, 200, 73, 70, 5…\n$ total        &lt;dbl&gt; 152, 126, 58, 160, 25, 210, 63, 110, 118, 163, 258, 84, 1…\n$ feature      &lt;chr&gt; \"adj\", \"adj\", \"adj\", \"adj\", \"adj\", \"adj\", \"adj\", \"adj\", \"…\n\n\nПредставим себе, что мы решили задаться целью найти наиболее диалектных носителей:\n\nlibrary(tidytext)\nmikhalevskaja |&gt; \n  mutate(ratio = conservative/total,\n         speaker = reorder_within(speaker, ratio, feature)) |&gt; \n  ggplot(aes(ratio, speaker, color = gender))+\n  geom_point()+\n  facet_wrap(~feature, scales = \"free\")+\n  scale_y_reordered()\n\n\n\n\n\n\n\n\nНе очень легко это анализировать… Давайте выберем один признак – подъем a: консервативными считались формы [ꞌpʲetʲero], а инновативной – реализация [ꞌpʲatʲərə]. Посчитаем долю и отсортируем:\n\nmikhalevskaja |&gt; \n  filter(feature == \"a-e\") |&gt; \n  mutate(ratio = conservative/total) |&gt; \n  arrange(desc(ratio))\n\n\n  \n\n\n\nВ целом, всего в интервью встречается от 4 до 244 контекстов для реализации признака. Хотим ли мы верить, что lpp1928 с 29 наблюдениями диалектнее, чем mgb1949 с 104 наблюдениями, только на основании доли?\n\nmikhalevskaja |&gt; \n  filter(feature  == \"a-e\") |&gt; \n  mutate(ratio = conservative/total) |&gt; \n  ggplot(aes(ratio))+\n  geom_histogram()+\n  xlim(-0.1, 1)\n\n`stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_bar()`).\n\n\n\n\n\n\n\n\n\nМы можем провести байесовский апдейт, но для этого нам нужно априорное распределение. Трюк, который предлагает байесовская эмпирическая оценка заключается в том, что априорное распределение можно попробовать получить на основании данных:\n\nmikhalevskaja |&gt; \n  filter(feature == \"a-e\") |&gt; \n  mutate(ratio = conservative/total) |&gt; \n  filter(ratio != 0, # оказывается fitdist плохо работает, когда много крайних точек\n         ratio != 1) -&gt;\n  for_beta_estimation \n  \nbeta_est &lt;- fitdistrplus::fitdist(for_beta_estimation$ratio, distr = 'beta', method = 'mle')\nbeta_est\n\nFitting of the distribution ' beta ' by maximum likelihood \nParameters:\n       estimate Std. Error\nshape1 1.590621  0.4127354\nshape2 5.445977  1.5766951\n\n\nСделаем байесовский апдейт:\n\nmikhalevskaja |&gt; \n  filter(feature == \"a-e\") |&gt; \n  mutate(alpha_prior = beta_est$estimate[1],\n         beta_prior = beta_est$estimate[2],\n         alpha_post = conservative+alpha_prior,\n         beta_post  = innovative+beta_prior,\n         mean_post = alpha_post/(alpha_post+beta_post),\n         ratio = conservative/total) |&gt; \n  ggplot(aes(ratio, mean_post, label = speaker, color = total))+\n  geom_hline(yintercept = beta_est$estimate[1]/sum(beta_est$estimate), linetype = 2)+\n  geom_point()+\n  ggrepel::geom_text_repel()\n\nWarning: ggrepel: 1 unlabeled data points (too many overlaps). Consider\nincreasing max.overlaps\n\n\n\n\n\n\n\n\n\nКак видно, байесовская оценка не сильно отличается от старой оценки средним, однако таким образом мы можем видеть, что после байесовского апдейта наблюдения с маленьким количеством наблюдений льнут к среднему априорного распределения. Мы можем даже умножить параметры нашего априорного распределения на 10, чтобы показать это:\n\nmikhalevskaja |&gt; \n  filter(feature == \"a-e\") |&gt; \n  mutate(alpha_prior = beta_est$estimate[1]*10,\n         beta_prior = beta_est$estimate[2]*10,\n         alpha_post = conservative+alpha_prior,\n         beta_post  = innovative+beta_prior,\n         mean_post = alpha_post/(alpha_post+beta_post),\n         ratio = conservative/total) |&gt; \n  ggplot(aes(ratio, mean_post, label = speaker, color = total))+\n  geom_hline(yintercept = beta_est$estimate[1]/sum(beta_est$estimate), linetype = 2)+\n  geom_point()+\n  ggrepel::geom_text_repel()\n\n\n\n\n\n\n\n\n\nmikhalevskaja |&gt; \n  filter(feature == \"a-e\") |&gt; \n  mutate(alpha_prior = beta_est$estimate[1]*40,\n         beta_prior = beta_est$estimate[2]*40,\n         alpha_post = conservative+alpha_prior,\n         beta_post  = innovative+beta_prior,\n         mean_post = alpha_post/(alpha_post+beta_post),\n         ratio = conservative/total) |&gt; \n  ggplot(aes(ratio, mean_post, label = speaker, color = total))+\n  geom_hline(yintercept = beta_est$estimate[1]/sum(beta_est$estimate), linetype = 2)+\n  geom_point()+\n  ggrepel::geom_text_repel()\n\n\n\n\n\n\n\n\nИли в формате гифки:\n\n\n\n\n\n\n\nВ работе (Coretta 2016) собраны данные длительности исландских гласных. Используя алгоритм максимального правдоподобия и идеи эмперической байесовской оценки, найдите априорное распределение для длительности гласных (переменная vowel.dur), используя все наблюдения в датасете и моделируя его нормальным распределением. Дальше проведите байесовский апдейт длительности гласных носителя tt01 (переменная speaker) и нарисуйте 80% доверительный интервал апостериорного распределения.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCoretta, Stefano. 2016. “Vowel Duration and Aspiration Effects in Icelandic.” University of York.\n\n\nDaniel, Michael, Ruprecht von Waldenfels, Aleksandra Ter-Avanesova, Polina Kazakova, Ilya Schurov, Ekaterina Gerasimenko, Daria Ignatenko, et al. 2019. “Dialect Loss in the Russian North: Modeling Change Across Variables.” Language Variation and Change 31 (3): 353–76. https://doi.org/10.1017/S0954394519000243.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Эмпирическая байесовская оценка</span>"
    ]
  },
  {
    "objectID": "08-a-b-testing.html",
    "href": "08-a-b-testing.html",
    "title": "8  A/B тестирование с пакетом bayesAB",
    "section": "",
    "text": "У нас есть два диалектолога, которые поехали в экспедицию. Оба записали интервью с одним и тем же носителем, тестируя, с какой вероятностью носитель выдаст диалектную форму (1) или литературную форму (0).\nРезультаты исследователя A: c(0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0)\nРезультаты исследователя B: c(0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\nУ нас есть подозрение, что на результаты исследователя А повлиял прайминг, который он не контролировал. Проверьте гипотезу, если мы, в общем, не ожидаем увидеть эту диалектную черту в данной местности (Beta(1, 10)).\n\n\n\n\nlibrary(bayesAB)\n\nA &lt;- c(0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0)\nB &lt;- c(0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n\nAB &lt;- bayesTest(A,\n                B,\n                priors = c('alpha' = 1, 'beta' = 10),\n                distribution = 'bernoulli')\n\nsummary(AB)\n\nQuantiles of posteriors for A and B:\n\n$Probability\n$Probability$A\n         0%         25%         50%         75%        100% \n0.006013801 0.084775503 0.115710476 0.152540026 0.437837821 \n\n$Probability$B\n          0%          25%          50%          75%         100% \n0.0008687924 0.0433149942 0.0661859395 0.0954168435 0.3577109647 \n\n\n--------------------------------------------\n\nP(A &gt; B) by (0)%: \n\n$Probability\n[1] 0.78612\n\n--------------------------------------------\n\nCredible Interval on (A - B) / B for interval length(s) (0.9) : \n\n$Probability\n        5%        95% \n-0.4492071  5.4585613 \n\n--------------------------------------------\n\nPosterior Expected Loss for choosing A over B:\n\n$Probability\n[1] 0.1276729\n\nplot(AB)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\nℹ The deprecated feature was likely used in the bayesAB package.\n  Please report the issue at &lt;https://github.com/FrankPortman/bayesAB/issues&gt;.\n\n\nWarning: `qplot()` was deprecated in ggplot2 3.4.0.\nℹ The deprecated feature was likely used in the bayesAB package.\n  Please report the issue at &lt;https://github.com/FrankPortman/bayesAB/issues&gt;.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>A/B тестирование с пакетом `bayesAB`</span>"
    ]
  },
  {
    "objectID": "09-Introduction-to-MCMC.html",
    "href": "09-Introduction-to-MCMC.html",
    "title": "9  Введение в Марковские цепи Монте-Карло",
    "section": "",
    "text": "9.1 Марковские цепи\nМарковская цепь Монте-Карло (Markov chain Monte Carlo, MCMC) — это класс алгоритмов для семплирования, которые позволяют моделировать некоторое распределение вероятностей. При моделировании используют разные алгоритмы, мы будем смотреть на примере алгоритма Метрополиса-Гастингса (Metropolis-Hastings).\nДля того, чтобы в этом разобраться нам потребуется обсудить:\nМарковский процесс\nВозьмем наш датасет с sms и посмотрим частоты разных частей речи::\nТеперь давайте посмотрим на частотность переходов из одних состояний в другие:\nИногда это визуализируют при помощи графов, но в нашем случае (это граф ham) это достаточно бесполезно (наводите на вершины стрелочки, чтобы что-то разглядеть):\nМожно посмотреть на славную визуализацию (спасибо за ссылку Марине Дубовой).",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Введение в Марковские цепи Монте-Карло</span>"
    ]
  },
  {
    "objectID": "09-Introduction-to-MCMC.html#марковские-цепи",
    "href": "09-Introduction-to-MCMC.html#марковские-цепи",
    "title": "9  Введение в Марковские цепи Монте-Карло",
    "section": "",
    "text": "конечное количество состояний\nвероятность переходов из одного состояния в другое",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Введение в Марковские цепи Монте-Карло</span>"
    ]
  },
  {
    "objectID": "09-Introduction-to-MCMC.html#хакерская-статистика-основано-на-статистика-для-хакеров-джейка-вандерпласа",
    "href": "09-Introduction-to-MCMC.html#хакерская-статистика-основано-на-статистика-для-хакеров-джейка-вандерпласа",
    "title": "9  Введение в Марковские цепи Монте-Карло",
    "section": "9.2 Хакерская статистика (основано на “Статистика для хакеров” Джейка Вандерпласа)",
    "text": "9.2 Хакерская статистика (основано на “Статистика для хакеров” Джейка Вандерпласа)\nВообще симуляции позволяют делать статистику более осмысленной.\n\n9.2.1 Биномиальные данные\n\n\n\n\n\n\nНемного упрощая данные из статьи (Rosenbach 2003: 394), можно сказать что носители британского английского предпочитают s-генитив (90%) of-генитиву (10%). Можно ли сказать, что перед Вами носитель британского английского, если Вы наблюдаете в интервью актера из 120 контекстов 100 s-генитивов?\n\n\n\nВероятность получить 100 успехов из 120 случаев, если мы верим, что вероятность успеха равна 0.9 описывается биномиальным распределением:\n\\[P(H = h|p, n) = \\binom{n}{h}\\times p^h\\times(1-p)^{1-h}\\]\nФреквентистский подход: биномиальный тест\n\nH\\(_0\\) человек говорит s-генитив с вероятностью 0.9\nα = 0.05\n\n\n\nWarning: Removed 107 rows containing non-finite outside the scale range\n(`stat_align()`).\n\n\n\n\n\n\n\n\n\nБайесовский подход: биномиальная функция правдоподобия перемножается с априорным бета распределением чтобы получить апостериорное распределение.\n\ntibble(h = seq(0, 1, 0.01),\n       y = dbeta(h, shape1 = 100 + 90, shape2 = 20 + 10)) |&gt; \n  ggplot(aes(h, y))+\n  geom_line()+\n  geom_area(aes(x = ifelse(h&gt;=qbeta(0.025, shape1 = 100 + 90, shape2 = 20 + 10) &\n                           h&lt;=qbeta(0.975, shape1 = 100 + 90, shape2 = 20 + 10), h, NA)), fill = \"darkgreen\")+\n  geom_vline(xintercept = 100/120, linetype = 2)\n\nWarning: Removed 92 rows containing non-finite outside the scale range\n(`stat_align()`).\n\n\n\n\n\n\n\n\n\nХакерский подход: симуляция:\n\nset.seed(42)\nmap_dbl(1:1000, function(i){\n  sample(0:1, 120, replace = TRUE, prob = c(0.1, 0.9)) |&gt; \n    sum()\n}) -&gt;\n  simulations\n\ntibble(sum = simulations) |&gt; \n  mutate(greater = sum &lt;= 100) |&gt; \n  group_by(greater) |&gt; \n  summarise(number = n())\n\n\n  \n\n\ntibble(sum = simulations) |&gt; \n  ggplot(aes(sum))+\n  geom_density()+\n  geom_vline(xintercept = 100, linetype = 2)+\n  scale_x_continuous(breaks = c(0:9*20), limits = c(0, 120))\n\n\n\n\n\n\n\n\nАналогично можно использовать:\n\nслучайное перемешивание вместо двухвыборочного t-теста;\nбутстрэп вместо одновыборочного t-теста.\n\n\n\n9.2.2 Метод Монте-Карло\nГруппа методов изучения случайных процессов, которые базируются на:\n\nвозможности производить бесконечного количества случайных значений\nдля известных или новых распределений\n\nПредставим себе, что у нас есть какой-то интеграл, который мы хотим посчитать. Например такой:\n\ntibble(x = seq(0, 1, length.out = 1000)) |&gt; \n  ggplot(aes(x))+\n  stat_function(fun = function(x){x^12-sin(x)+1}, geom = \"area\", fill = \"lightblue\")\n\n\n\n\n\n\n\n\nМы можем насэмплировать точек из комбинации двух унимодальных распределений u(0, 1), и u(0, 1.2) и посмотреть, кто попадает в область, а кто нет:\n\nset.seed(42)\ntibble(x = runif(1e3, 0, 1), \n       y = runif(1e3, 0, 1.2),\n       in_area = y &lt; x^12-sin(x)+1) |&gt; \n  ggplot(aes(x, y))+\n  stat_function(fun = function(x){x^12-sin(x)+1}, geom = \"area\", fill = \"lightblue\")+\n  geom_point(aes(color = in_area), show.legend = FALSE)\n\n\n\n\n\n\n\n\nСколько попало?\n\nset.seed(42)\ntibble(x = runif(1e3, 0, 1), \n       y = runif(1e3, 0, 1.2),\n       in_area = y &lt; x^12-sin(x)+1) |&gt; \n  count(in_area)\n\n\n  \n\n\n528/1000*1.2\n\n[1] 0.6336\n\nintegrate(function(x){x^12-sin(x)+1}, 0, 1)\n\n0.6172254 with absolute error &lt; 6.9e-15\n\n\nА если увеличить количество наблюдений?\n\nset.seed(42)\ntibble(x = runif(1e7, 0, 1), \n       y = runif(1e7, 0, 1.2),\n       in_area = y &lt; x^12-sin(x)+1) |&gt; \n  count(in_area)\n\n\n  \n\n\n5143667/10000000*1.2\n\n[1] 0.61724\n\nintegrate(function(x){x^12-sin(x)+1}, 0, 1)\n\n0.6172254 with absolute error &lt; 6.9e-15",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Введение в Марковские цепи Монте-Карло</span>"
    ]
  },
  {
    "objectID": "09-Introduction-to-MCMC.html#соединение-идей-марковских-цепей-и-монте-карло",
    "href": "09-Introduction-to-MCMC.html#соединение-идей-марковских-цепей-и-монте-карло",
    "title": "9  Введение в Марковские цепи Монте-Карло",
    "section": "9.3 Соединение идей Марковских цепей и Монте-Карло",
    "text": "9.3 Соединение идей Марковских цепей и Монте-Карло\n\nshiny::runGitHub(\"agricolamz/mcmc_shiny\")\n\nОсновные проблемы MCMC:\n\nЗависимость от начального значения. Решение: выкинуть начальную часть цепи (burn-in).\nПолученные значения автокоррелируют, так как они были получины при помощи марковского процесса. Решение: брать, например, каждое третье значение.\n\nВы можете почитать историю идей MCMC в работе (Robert and Casella 2011) (доступна здесь).",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Введение в Марковские цепи Монте-Карло</span>"
    ]
  },
  {
    "objectID": "09-Introduction-to-MCMC.html#brms",
    "href": "09-Introduction-to-MCMC.html#brms",
    "title": "9  Введение в Марковские цепи Монте-Карло",
    "section": "9.4 brms",
    "text": "9.4 brms\nДля вычисления всяких сложных статистических моделей люди придумали вероятностные языки программирования. Чаще всего они являются расширением для стандартных языков программирования, но иногда становятся самодостаточными языками (однако они все равно написаны на каких-то быстрых языках программирования типа C++). Примерами таких самодостаточных языков является:\n\nBUGS (Bayesian inference Using Gibbs Sampling)\nJAGS (Just Another Gibbs Sampler)\nNIMBLE\nNUTS (No-U-Turn-Sampler)\nStan\n\nДля них пишут обертки на разных языках, мы будем использовать пакет brms, который является оберткой над пакетом rstan, который является оберткой для Stan. Вы можете установить эти пакеты к себе на компьютер, но есть высокая вероятность, что что-то пойдет не так, в связи с чем, я предлагаю всем использовать rstudio.cloud для примеров на brms.\nЧтобы повторить примеры Вам нужно:\n\nУстановить на свой компьютер rstan (инструкции) и brms\n\nЕсли на вашем компьютере не выходит, попробуйте &lt;posit.cloud&gt;\n\n9.4.1 Регрессионный пример\n\n\n\n\n\n\nВ работе (Coretta 2016) собраны данные длительности исландских гласных. Отфильтруйте данные, произнесенные носителем tt01 (переменная speaker), и смоделируйте длительность гласных (переменная vowel.dur) нормальным распределением используя в качестве априорного распределения нормальное распределение со средним 275 и стандартным отклонением 65 (основано на (Hillenbrand et al. 1995)).\n\n\n\n\nread_csv(\"https://raw.githubusercontent.com/agricolamz/2024_HSE_b_da4l/master/data/Coretta_2017_icelandic.csv\") |&gt; \n  filter(speaker == \"tt01\") -&gt;\n  vowel_data\n\nВ качестве первого этапа стоит сформулировать модель и посмотреть, в какой форме от нас ожидают априорное распределение.\n\nlibrary(brms)\nget_prior(vowel.dur ~ 0 + Intercept, \n          family = \"normal\",\n          data = vowel_data)\n\n\n  \n\n\n\nСделаем нашу первую модель:\n\nnormal_fit &lt;- brm(vowel.dur ~ 0 + Intercept, \n                  family = \"normal\",\n                  data = vowel_data,\n                  prior = c(prior(normal(275, 65), coef = Intercept)),\n                  silent = TRUE)\n\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 8e-06 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.08 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.017 seconds (Warm-up)\nChain 1:                0.01 seconds (Sampling)\nChain 1:                0.027 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 3e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.03 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.014 seconds (Warm-up)\nChain 2:                0.01 seconds (Sampling)\nChain 2:                0.024 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 2e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.02 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.014 seconds (Warm-up)\nChain 3:                0.011 seconds (Sampling)\nChain 3:                0.025 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 2e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.02 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.014 seconds (Warm-up)\nChain 4:                0.009 seconds (Sampling)\nChain 4:                0.023 seconds (Total)\nChain 4: \n\nnormal_fit\n\n Family: gaussian \n  Links: mu = identity \nFormula: vowel.dur ~ 0 + Intercept \n   Data: vowel_data (Number of observations: 175) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    89.57      1.94    85.87    93.53 1.00     2804     2296\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    24.96      1.32    22.57    27.80 1.00     2921     2730\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nВизуализируем нашу модель:\n\nplot(normal_fit)\n\n\n\n\n\n\n\n\nПосле того как мы сгенерировали наши распределения, мы можем их достать и что-нибудь посчитать:\n\nas_draws_array(normal_fit) |&gt; \n  posterior::summarize_draws()\n\n\n  \n\n\nas_draws_rvars(normal_fit)\n\n# A draws_rvars: 1000 iterations, 4 chains, and 4 variables\n$b_Intercept: rvar&lt;1000,4&gt;[1] mean ± sd:\n[1] 90 ± 1.9 \n\n$sigma: rvar&lt;1000,4&gt;[1] mean ± sd:\n[1] 25 ± 1.3 \n\n$lprior: rvar&lt;1000,4&gt;[1] mean ± sd:\n[1] -13 ± 0.12 \n\n$lp__: rvar&lt;1000,4&gt;[1] mean ± sd:\n[1] -822 ± 1 \n\n\nМожно визуально оценить, где лежат наши данные, а что предсказывает полученная модель:\n\npp_check(normal_fit)\n\n\n\n\n\n\n\n\n\n\n\n\nCoretta, Stefano. 2016. “Vowel Duration and Aspiration Effects in Icelandic.” University of York.\n\n\nHillenbrand, James, Laura A Getty, Michael J Clark, and Kimberlee Wheeler. 1995. “Acoustic Characteristics of American English Vowels.” The Journal of the Acoustical Society of America 97 (5): 3099–3111.\n\n\nRobert, C., and G. Casella. 2011. “A Short History of Markov Chain Monte Carlo: Subjective Recollections from Incomplete Data.” Statistical Science 26 (1): 102–15.\n\n\nRosenbach, Anette. 2003. “Aspects of Iconicity and Economy in the Choice Between the s-Genitive and the of-Genitive in English.” In Determinants of Grammatical Variation in English, edited by Günter Rohdenburg and Britta Mondorf. Berlin, New York: Mouton de Gruyter.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Введение в Марковские цепи Монте-Карло</span>"
    ]
  },
  {
    "objectID": "10-advanced-regression.html",
    "href": "10-advanced-regression.html",
    "title": "10  Ограничения на применение регрессии",
    "section": "",
    "text": "10.1 Дисперсия и стандартное отклонение\nДисперсия — мера разброса значений наблюдений относительно среднего.\n\\[\\sigma^2_X = \\frac{\\sum_{i = 1}^n(x_i - \\bar{x})^2}{n - 1},\\]\nгде\nПредставим, что у нас есть следующие данные:\nТогда дисперсия — это сумма квадратов расстояний от каждой точки до среднего выборки (пунктирная линия) разделенное на количество наблюдений - 1 (по духу эта мера — обычное среднее, но если вас инетересует разница смещенной и несмещенной оценки дисперсии, см. видео).\nДля того чтобы было понятнее, что такое дисперсия, давайте рассмотрим несколько расспределений с одним и тем же средним, но разными дисперсиями:\nВ R дисперсию можно посчитать при помощи функции var()1.\nset.seed(42)\nx &lt;- rnorm(20, mean = 50, sd = 10)\nvar(x)\n\n[1] 172.2993\nПроверим, что функция выдает то же, что мы записали в формуле.\nvar(x) == sum((x - mean(x))^2)/(length(x)-1)\n\n[1] TRUE\nТак как дисперсия является квадратом отклонения, то часто вместо нее используют более интерпретируемое стандартное отклонение \\(\\sigma\\) — корень из дисперсии. В R ее можно посчитать при помощи функции sd():\nsd(x)\n\n[1] 13.12628\n\nsd(x) == sqrt(var(x))\n\n[1] TRUE",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Ограничения на применение регрессии</span>"
    ]
  },
  {
    "objectID": "10-advanced-regression.html#дисперсия-и-стандартное-отклонение",
    "href": "10-advanced-regression.html#дисперсия-и-стандартное-отклонение",
    "title": "10  Ограничения на применение регрессии",
    "section": "",
    "text": "\\(x_1, ..., x_n\\) — наблюдения;\n\\(\\bar{x}\\) — среднее всех наблюдений;\n\\(X\\) — вектор всех наблюдений;\n\\(n\\) — количество наблюдений.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Ограничения на применение регрессии</span>"
    ]
  },
  {
    "objectID": "10-advanced-regression.html#ковариация",
    "href": "10-advanced-regression.html#ковариация",
    "title": "10  Ограничения на применение регрессии",
    "section": "10.2 Ковариация",
    "text": "10.2 Ковариация\nКовариация — эта мера ассоциации двух переменных.\n\\[cov(X, Y) = \\frac{\\sum_{i = 1}^n(x_i - \\bar{x})(y_i-\\bar{y})}{n - 1},\\]\nгде\n\n\\((x_1, y_1), ..., (x_n, y_n)\\) — пары наблюдений;\n\\(\\bar{x}, \\bar{y}\\) — средние наблюдений;\n\\(X, Y\\) — векторы всех наблюдений;\n\\(n\\) — количество наблюдений.\n\nПредставим, что у нас есть следующие данные:\n\n\n\n\n\n\n\n\n\nТогда, согласно формуле, для каждой точки вычисляется следующая площадь (пуктирными линиями обозначены средние):\n\n\nWarning in geom_text(aes(x = mean(x) + 4), y = 26, label = \"среднее x\", : All aesthetics have length 1, but the data has 30 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_text(aes(y = mean(y) + 2), x = 25, label = \"среднее y\", : All aesthetics have length 1, but the data has 30 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n\n\nЕсли значения \\(x_i\\) и \\(y_i\\) какой-то точки либо оба больше, либо оба меньше средних \\(\\bar{x}\\) и \\(\\bar{y}\\), то получившееся произведение будет иметь знак +, если же наоборот — знак -. На графике это показано цветом.\n\n\nWarning in geom_text(aes(x = mean(x) + 4), y = 26, label = \"среднее x\", : All aesthetics have length 1, but the data has 30 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_text(aes(y = mean(y) + 2), x = 25, label = \"среднее y\", : All aesthetics have length 1, but the data has 30 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n\n\nТаким образом, если много красных прямоугольников, то значение суммы будет положительное и обозначать положительную связь (чем больше \\(x\\), тем больше \\(y\\)), а если будет много синий прямоугольников, то значение суммы отрицательное и обозначать положительную связь (чем больше \\(x\\), тем меньше \\(y\\)). Непосредственно значение ковариации не очень информативно, так как может достаточно сильно варьироваться от датасета к датасету.\nВ R ковариацию можно посчитать при помощи функции cov().\n\nset.seed(42)\nx &lt;- rnorm(10, mean = 50, sd = 10)\ny &lt;-  x + rnorm(10, sd = 10)\ncov(x, y)\n\n[1] 18.72204\n\ncov(x, -y*2)\n\n[1] -37.44407\n\n\nКак видно, простое умножение на два удвоило значение ковариации, что показывает, что непосредственно ковариацию использовать для сравнения разных датасетов не стоит.\nПроверим, что функция выдает то же, что мы записали в формуле.\n\ncov(x, y) == sum((x-mean(x))*(y - mean(y)))/(length(x)-1)\n\n[1] TRUE",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Ограничения на применение регрессии</span>"
    ]
  },
  {
    "objectID": "10-advanced-regression.html#корреляция",
    "href": "10-advanced-regression.html#корреляция",
    "title": "10  Ограничения на применение регрессии",
    "section": "10.3 Корреляция",
    "text": "10.3 Корреляция\nКорреляция — это мера ассоциации/связи двух числовых переменных. Помните, что бытовое применение этого термина к категориальным переменным (например, корреляция цвета глаз и успеваемость на занятиях по R) не имеет смысла с точки зрения статистики.\n\n10.3.1 Корреляция Пирсона\nКоэффициент корреляции Пирсона — базовый коэффициент ассоциации переменных, однако стоит помнить, что он дает неправильную оценку, если связь между переменными нелинейна.\n\\[\\rho_{X,Y} = \\frac{cov(X, Y)}{\\sigma_X\\times\\sigma_Y} = \\frac{1}{n-1}\\times\\sum_{i = 1}^n\\left(\\frac{x_i-\\bar{x}}{\\sigma_X}\\times\\frac{y_i-\\bar{y}}{\\sigma_Y}\\right),\\]\nгде\n\n\\((x_1, y_1), ..., (x_n, y_n)\\) — пары наблюдений;\n\\(\\bar{x}, \\bar{y}\\) — средние наблюдений;\n\\(X, Y\\) — векторы всех наблюдений;\n\\(n\\) — количество наблюдений.\n\nПоследнее уравнение показывает, что коэффициент корреляции Пирсона можно представить как среднее (с поправкой, поэтому \\(n-1\\), а не \\(n\\)) произведение \\(z\\)-нормализованных значений двух переменных.\n\n\nWarning in geom_text(aes(x = mean(x) + 0.8), y = -2, label = \"нормализованное среднее x\", : All aesthetics have length 1, but the data has 30 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_text(aes(y = mean(y) + 0.1), x = -1.6, label = \"нормализованное среднее y\", : All aesthetics have length 1, but the data has 30 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n\n\nЭта нормализация приводит к тому, что\n\nзначения корреляции имеют те же свойства знака коэффициента что и ковариация:\n\nесли коэффициент положительный (т. е. много красных прямоугольников) — связь между переменными положительная (чем больше \\(x\\), тем больше \\(y\\)),\nесли коэффициент отрицательный (т. е. много синих прямоугольников) — связь между переменными отрицательная (чем больше \\(x\\), тем меньше \\(y\\));\n\nзначение корреляции имееет независимое от типа данных интеретация:\n\nесли модуль коэффициента близок к 1 или ему равен — связь между переменными сильная,\nесли модуль коэффициента близок к 0 или ему равен — связь между переменными слабая.\n\n\nДля того чтобы было понятнее, что такое корреляция, давайте рассмотрим несколько распределений с разными значениями корреляции:\n\n\n\n\n\n\n\n\n\nКак видно из этого графика, чем ближе модуль корреляции к 1, тем боллее компактно расположены точки друг к другу, чем ближе к 0, тем более рассеяны значения. Достаточно легко научиться приблизительно оценивать коэфициент корреляции на глаз, поиграв 2–5 минут в игру “Угадай корреляцию” здесь или здесь.\nВ R коэффициент корреляции Пирсона можно посчитать при помощи функции cor().\n\nset.seed(42)\nx &lt;- rnorm(15, mean = 50, sd = 10)\ny &lt;-  x + rnorm(15, sd = 10)\ncor(x, y)\n\n[1] 0.6659041\n\n\nПроверим, что функция выдает то же, что мы записали в формуле.\n\ncor(x, y) == cov(x, y)/(sd(x)*sd(y))\n\n[1] TRUE\n\ncor(x, y) == sum(scale(x)*scale(y))/(length(x)-1)\n\n[1] TRUE",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Ограничения на применение регрессии</span>"
    ]
  },
  {
    "objectID": "10-advanced-regression.html#основы-регрессионного-анализа",
    "href": "10-advanced-regression.html#основы-регрессионного-анализа",
    "title": "10  Ограничения на применение регрессии",
    "section": "10.4 Основы регрессионного анализа",
    "text": "10.4 Основы регрессионного анализа\n\n\n\n\n\n\n\n\n\nКогда мы пытаемся научиться предсказывать данные одной переменной \\(Y\\) при помощи другой переменной \\(X\\), мы получаем формулу:\n\\[y_i = \\hat\\beta_0 + \\hat\\beta_1 \\times x_i + \\epsilon_i,\\] где\n\n\\(x_i\\) — \\(i\\)-ый элемент вектора значений \\(X\\);\n\\(y_i\\) — \\(i\\)-ый элемент вектора значений \\(Y\\);\n\\(\\hat\\beta_0\\) — оценка случайного члена (intercept);\n\\(\\hat\\beta_1\\) — оценка углового коэффициента (slope);\n\\(\\epsilon_i\\) — \\(i\\)-ый остаток, разница между оценкой модели (\\(\\hat\\beta_0 + \\hat\\beta_1 \\times x_i\\)) и реальным значением \\(y_i\\); весь вектор остатков иногда называют случайным шумом (на графике выделены красным).\n\nПричем, иногда мы можем один или другой параметр считать равным нулю.\n\n\n\n\n\n\nОпределите по графику формулу синей прямой.\n\n\n\n\n\n\n\n\n\n\n\n\nЗадача регрессии — оценить параметры \\(\\hat\\beta_0\\) и \\(\\hat\\beta_1\\), если нам известны все значения \\(x_i\\) и \\(y_i\\) и мы пытаемся минимизировать значния \\(\\epsilon_i\\). В данном конкретном случае, задачу можно решить аналитически и получить следующие формулы:\n\\[\\hat\\beta_1 = \\frac{(\\sum_{i=1}^n x_i\\times y_i)-n\\times\\bar x \\times \\bar y}{\\sum_{i = 1}^n(x_i-\\bar x)^2}\\]\n\\[\\hat\\beta_0 = \\bar y - \\hat\\beta_1\\times\\bar x\\]\nПри этом, вне зависимости от статистической школы, у регрессии есть свои ограничения на применение:\n\nлинейность связи между \\(x\\) и \\(y\\);\nнормальность распределение остатков \\(\\epsilon_i\\);\nгомоскидастичность — равномерность распределения остатков на всем протяжении \\(x\\);\nнезависимость переменных;\nнезависимость наблюдений друг от друга.\n\n\n10.4.1 Первая регрессия\nДавайте попробуем смоделировать количество слов и в рассказах М. Зощенко в зависимости от длины рассказа:\n\nzo &lt;- read_tsv(\"https://github.com/agricolamz/DS_for_DH/raw/master/data/tidy_zoshenko.csv\")\n\nzo |&gt; \n  filter(word == \"и\") |&gt; \n  distinct() |&gt; \n  ggplot(aes(n_words, n))+\n  geom_point()+\n  labs(x = \"количество слов в рассказе\",\n       y = \"количество и\")\n\n\n\n\n\n\n\n\nМы видим, несколько одиночных точек, давайте избавимся от них и добавим регрессионную линию при помощи функции geom_smooth():\n\nzo |&gt; \n  filter(word == \"и\",\n         n_words &lt; 1500) |&gt; \n  distinct() -&gt;\n  zo_filtered\n\nzo_filtered |&gt;   \n  ggplot(aes(n_words, n))+\n  geom_point()+\n  geom_smooth(method = \"lm\", se = FALSE)+\n  labs(x = \"количество слов в рассказе\",\n       y = \"количество и\")\n\n\n\n\n\n\n\n\nЧтобы получить формулу этой линии нужно запустить функцию, которая оценивает линейную регрессию:\n\nfit &lt;- lm(n~n_words, data = zo_filtered)\nfit\n\n\nCall:\nlm(formula = n ~ n_words, data = zo_filtered)\n\nCoefficients:\n(Intercept)      n_words  \n   -1.47184      0.04405  \n\n\nВот мы и получили коэффициенты, теперь мы видим, что наша модель считает следующее:\n\\[n = -1.47184 + 0.04405 \\times n\\_words\\]\nБолее подробную информцию можно посмотреть, если запустить модель в функцию summary():\n\nsummary(fit)\n\n\nCall:\nlm(formula = n ~ n_words, data = zo_filtered)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-19.6830  -4.3835   0.8986   4.6486  19.6413 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -1.471840   2.467149  -0.597    0.553    \nn_words      0.044049   0.003666  12.015   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.945 on 64 degrees of freedom\nMultiple R-squared:  0.6928,    Adjusted R-squared:  0.688 \nF-statistic: 144.4 on 1 and 64 DF,  p-value: &lt; 2.2e-16\n\n\nВ разделе Coefficients содержится информацию про наши коэффициенты:\n\nEstimate – полученная оценка коэффициентов;\nStd. Error – стандартная ошибка среднего;\nt value – \\(t\\)-статистика, полученная при проведении одновыборочного \\(t\\)-теста, сравнивающего данный коэфициент с 0;\nPr(&gt;|t|) – полученное \\(p\\)-значение;\nMultiple R-squared и Adjusted R-squared — одна из оценок модели, показывает связь между переменными. Без поправок совпадает с квадратом коэффициента корреляции Пирсона:\n\n\ncor(zo_filtered$n_words, zo_filtered$n)^2\n\n[1] 0.6928376\n\n\n\nF-statistic — \\(F\\)-статистика полученная при проведении теста, проверяющего, не являются ли хотя бы один из коэффицинтов статистически значимо отличается от нуля. Совпадает с результатами дисперсионного анализа (ANOVA).\n\nТеперь мы можем даже предсказывать значения, которые мы еще не видели. Например, сколько будет и в рассказе Зощенко длиной 1000 слов?\n\n\n\n\n\n\n\n\n\n\npredict(fit, tibble(n_words = 1000))\n\n       1 \n42.57715 \n\n\n\n\n10.4.2 Категориальные переменные\nЧто если мы хотим включить в наш анализ категориальные переменные? Давайте рассмотрим простой пример с рассказами Чехова и Зощенко, которые мы рассматривали в прошлом разделе. Мы будем анализировать логарифм доли слов деньги:\n\nchekhov &lt;- read_tsv(\"https://github.com/agricolamz/DS_for_DH/raw/master/data/tidy_chekhov.tsv\")\nzoshenko &lt;- read_tsv(\"https://github.com/agricolamz/DS_for_DH/raw/master/data/tidy_zoshenko.csv\")\n\nchekhov$author &lt;- \"Чехов\"\nzoshenko$author &lt;- \"Зощенко\"\n\nchekhov |&gt; \n  bind_rows(zoshenko) |&gt; \n  filter(str_detect(word, \"деньг\")) |&gt; \n  group_by(author, titles, n_words) |&gt; \n  summarise(n = sum(n)) |&gt; \n  mutate(log_ratio = log(n/n_words)) -&gt;\n  checkov_zoshenko\n\nВизуализация выглядит так:\n\n\n\n\n\n\n\n\n\nКрасной точкой обозначены средние значения, так что мы видим, что между двумя писателями есть разница, но является ли она статистически значимой? В прошлом разделе, мы рассмотрели, что в таком случае можно сделать t-test:\n\nt.test(log_ratio~author, \n       data = checkov_zoshenko, \n       var.equal =TRUE) # здесь я мухлюю, отключая поправку Уэлча\n\n\n    Two Sample t-test\n\ndata:  log_ratio by author\nt = 5.6871, df = 125, p-value = 8.665e-08\nalternative hypothesis: true difference in means between group Зощенко and group Чехов is not equal to 0\n95 percent confidence interval:\n 0.8606107 1.7793181\nsample estimates:\nmean in group Зощенко   mean in group Чехов \n            -5.021262             -6.341226 \n\n\nРазница между группами является статистически значимой (t(125) = 5.6871, p-value = 8.665e-08).\nДля того, чтобы запустить регрессию на категориальных данных категориальная переменная автоматически разбивается на группу бинарных dummy-переменных:\n\ntibble(author = c(\"Чехов\", \"Зощенко\"),\n       dummy_chekhov = c(1, 0),\n       dummy_zoshenko = c(0, 1))\n\n\n  \n\n\n\nДальше для регрессионного анализа выкидывают одну из переменных, так как иначе модель не сойдется (dummy-переменных всегда n-1, где n — количество категорий в переменной).\n\ntibble(author = c(\"Чехов\", \"Зощенко\"),\n       dummy_chekhov = c(1, 0))\n\n\n  \n\n\n\nЕсли переменная dummy_chekhov принимает значение 1, значит речь о рассказе Чехова, а если принимает значение 0, то о рассказе Зощенко. Если вставить нашу переменную в регрессионную формулу получится следующее:\n\\[y_i = \\hat\\beta_0 + \\hat\\beta_1 \\times \\text{dummy-chekhov} + \\epsilon_i,\\]\nТак как dummy_chekhov принимает либо значение 1, либо значение 0, то получается, что модель предсказывает лишь два значения:\n\\[y_i = \\left\\{\\begin{array}{ll}\\hat\\beta_0 + \\hat\\beta_1 \\times 1 + \\epsilon_i = \\hat\\beta_0 + \\hat\\beta_1 + \\epsilon_i\\text{, если рассказ Чехова}\\\\\n\\hat\\beta_0 + \\hat\\beta_1 \\times 0 + \\epsilon_i = \\hat\\beta_0 + \\epsilon_i\\text{, если рассказ Зощенко}\n\\end{array}\\right.\\]\nТаким образом, получается, что свободный член \\(\\beta_0\\) и угловой коэффициент \\(\\beta_1\\) в регресси с категориальной переменной получает другую интерпретацию. Одно из значений переменной кодируется при помощи \\(\\beta_0\\), а сумма коэффициентов \\(\\beta_0+\\beta_1\\) дают другое значение переменной. Так что \\(\\beta_1\\) — это разница между оценками двух значений переменной.\nДавайте теперь запустим регрессию на этих же данных:\n\nfit2 &lt;- lm(log_ratio~author, data = checkov_zoshenko)\nsummary(fit2)\n\n\nCall:\nlm(formula = log_ratio ~ author, data = checkov_zoshenko)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.8652 -0.6105 -0.0607  0.6546  3.2398 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -5.0213     0.2120 -23.680  &lt; 2e-16 ***\nauthorЧехов  -1.3200     0.2321  -5.687 8.67e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9717 on 125 degrees of freedom\nMultiple R-squared:  0.2056,    Adjusted R-squared:  0.1992 \nF-statistic: 32.34 on 1 and 125 DF,  p-value: 8.665e-08\n\n\nВо-первых стоит обратить внимание на то, что R сам преобразовал нашу категориальную переменную в dummy-переменную authorЧехов. Во-вторых, можно заметить, что значения t-статистики и p-value совпадают с результатами полученными нами в t-тесте выше. Статистическти значимый коэффициент при аргументе authorЧехов следует интерпретировать как разницу средних между логарифмом долей в рассказах Чехова и Зощенко.\n\n\n10.4.3 Множественная регрессия\nМножественная регрессия позволяет проанализировать связь между зависимой и несколькими зависимыми переменными. Формула множественной регрессии не сильно отличается от формулы обычной линейной регрессии:\n\\[y_i = \\hat\\beta_0 + \\hat\\beta_1 \\times x_{1i}+ \\dots+ \\hat\\beta_n \\times x_{ni} + \\epsilon_i,\\]\n\n\\(x_{ki}\\) — \\(i\\)-ый элемент векторов значений \\(X_1, \\dots, X_n\\);\n\\(y_i\\) — \\(i\\)-ый элемент вектора значений \\(Y\\);\n\\(\\hat\\beta_0\\) — оценка случайного члена (intercept);\n\\(\\hat\\beta_k\\) — коэфциент при переменной \\(X_{k}\\);\n\\(\\epsilon_i\\) — \\(i\\)-ый остаток, разница между оценкой модели (\\(\\hat\\beta_0 + \\hat\\beta_1 \\times x_i\\)) и реальным значением \\(y_i\\); весь вектор остатков иногда называют случайным шумом.\n\nВ такой регресии предикторы могут быть как числовыми, так и категориальными (со всеми вытекающими последствиями, которые мы обсудили в предудщем разделе). Такую регрессию чаще всего сложно визуализировать, так как в одну регрессионную линию вкладываются сразу несколько переменных.\nПопробуем предсказать длину лепестка на основе длины чашелистик и вида ириса:\n\niris |&gt; \n  ggplot(aes(Sepal.Length, Petal.Length, color = Species))+\n  geom_point()\n\n\n\n\n\n\n\n\nЗапустим регрессию:\n\nfit3 &lt;- lm(Petal.Length ~ Sepal.Length+ Species, data = iris)\nsummary(fit3)\n\n\nCall:\nlm(formula = Petal.Length ~ Sepal.Length + Species, data = iris)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.76390 -0.17875  0.00716  0.17461  0.79954 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       -1.70234    0.23013  -7.397 1.01e-11 ***\nSepal.Length       0.63211    0.04527  13.962  &lt; 2e-16 ***\nSpeciesversicolor  2.21014    0.07047  31.362  &lt; 2e-16 ***\nSpeciesvirginica   3.09000    0.09123  33.870  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2826 on 146 degrees of freedom\nMultiple R-squared:  0.9749,    Adjusted R-squared:  0.9744 \nF-statistic:  1890 on 3 and 146 DF,  p-value: &lt; 2.2e-16\n\n\nВсе предикторы статистически значимы. Давайте посмотрим предсказания модели для всех наблюдений:\n\niris |&gt; \n  mutate(prediction = predict(fit3)) |&gt; \n  ggplot(aes(Sepal.Length, prediction, color = Species))+\n  geom_point()\n\n\n\n\n\n\n\n\nВсегда имеет смысл визуализировать, что нам говорит наша модель. Если использовать пакет ggeffects (или предшествовавший ему пакет effects), это можно сделать не сильно задумываясь, как это делать:\n\nlibrary(ggeffects)\nplot(ggpredict(fit3, terms = c(\"Sepal.Length\", \"Species\")))\n\n\n\n\n\n\n\n\nКак видно из графиков, наша модель имеет одинаковые угловые коэффициенты (slope) для каждого из видов ириса и разные свободные члены (intercept).\n\nsummary(fit3)\n\n\nCall:\nlm(formula = Petal.Length ~ Sepal.Length + Species, data = iris)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.76390 -0.17875  0.00716  0.17461  0.79954 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       -1.70234    0.23013  -7.397 1.01e-11 ***\nSepal.Length       0.63211    0.04527  13.962  &lt; 2e-16 ***\nSpeciesversicolor  2.21014    0.07047  31.362  &lt; 2e-16 ***\nSpeciesvirginica   3.09000    0.09123  33.870  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2826 on 146 degrees of freedom\nMultiple R-squared:  0.9749,    Adjusted R-squared:  0.9744 \nF-statistic:  1890 on 3 and 146 DF,  p-value: &lt; 2.2e-16\n\n\n\\[y_i = \\left\\{\\begin{array}{ll} -1.70234 + 0.63211 \\times \\text{Sepal.Length} + \\epsilon_i\\text{, если вид setosa}\\\\\n-1.70234 + 2.2101 + 0.63211 \\times \\text{Sepal.Length} + \\epsilon_i\\text{, если вид versicolor} \\\\\n-1.70234 + 3.09 + 0.63211 \\times \\text{Sepal.Length} + \\epsilon_i\\text{, если вид virginica}\n\\end{array}\\right.\\]",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Ограничения на применение регрессии</span>"
    ]
  },
  {
    "objectID": "10-advanced-regression.html#нелинейность-взаимосвязи",
    "href": "10-advanced-regression.html#нелинейность-взаимосвязи",
    "title": "10  Ограничения на применение регрессии",
    "section": "10.5 Нелинейность взаимосвязи",
    "text": "10.5 Нелинейность взаимосвязи\nДавайте восползуемся данными из пакета Rling Натальи Левшиной. В датасете 100 произвольно выбранных слов из проекта English Lexicon Project (Balota et al. 2007), их длина, среднее время реакции и частота в корпусе.\n\nldt &lt;- read_csv(\"https://raw.githubusercontent.com/agricolamz/2024_HSE_b_da4l/main/data/ldt.csv\")\nldt\n\n\n  \n\n\n\nДавайте посмотрим на простой график:\n\nldt |&gt; \n  ggplot(aes(Mean_RT, Freq))+\n  geom_point()+\n  theme_bw()\n\n\n\n\n\n\n\n\nРегрессия на таких данных будет супер неиформативна:\n\nldt |&gt; \n  ggplot(aes(Mean_RT, Freq))+\n  geom_point()+\n  geom_smooth(method = \"lm\")+\n  theme_bw()\n\n\n\n\n\n\n\nm1 &lt;- summary(lm(Mean_RT~Freq, data = ldt))\nm1\n\n\nCall:\nlm(formula = Mean_RT ~ Freq, data = ldt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-224.93  -85.42  -30.52   81.90  632.66 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 826.998242  15.229783  54.301  &lt; 2e-16 ***\nFreq         -0.005595   0.001486  -3.765 0.000284 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 143.9 on 98 degrees of freedom\nMultiple R-squared:  0.1264,    Adjusted R-squared:  0.1174 \nF-statistic: 14.17 on 1 and 98 DF,  p-value: 0.0002843\n\n\n\n10.5.1 Логарифмирование\n\nldt |&gt; \n  ggplot(aes(Mean_RT, log(Freq)))+\n  geom_point()+\n  geom_smooth(method = \"lm\")+\n  theme_bw()\n\nWarning: Removed 5 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\n\n\n\n\n\n\nldt |&gt; \n  ggplot(aes(Mean_RT, log(Freq+1)))+\n  geom_point()+\n  geom_smooth(method = \"lm\")+\n  theme_bw()\n\n\n\n\n\n\n\nm2 &lt;- summary(lm(Mean_RT~log(Freq+1), data = ldt))\nm2\n\n\nCall:\nlm(formula = Mean_RT ~ log(Freq + 1), data = ldt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-242.36  -76.66  -17.49   48.64  630.49 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    1001.60      29.79  33.627  &lt; 2e-16 ***\nlog(Freq + 1)   -34.03       4.76  -7.149 1.58e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 124.8 on 98 degrees of freedom\nMultiple R-squared:  0.3428,    Adjusted R-squared:  0.3361 \nF-statistic: 51.11 on 1 and 98 DF,  p-value: 1.576e-10\n\nm1$adj.r.squared\n\n[1] 0.1174405\n\nm2$adj.r.squared\n\n[1] 0.336078\n\n\nОтлогорифмировать можно и другую переменную.\n\nldt |&gt; \n  ggplot(aes(log(Mean_RT), log(Freq  + 1)))+\n  geom_point()+\n  geom_smooth(method = \"lm\")+\n  theme_bw()\n\n\n\n\n\n\n\nm3 &lt;- summary(lm(log(Mean_RT)~log(Freq+1), data = ldt))\nm1$adj.r.squared\n\n[1] 0.1174405\n\nm2$adj.r.squared\n\n[1] 0.336078\n\nm3$adj.r.squared\n\n[1] 0.3838649\n\n\nКак интерпретировать полученную регрессию с двумя отлогорифмированными значениями?\nВ обычной линейной регресии мы узнаем отношения между \\(x\\) и \\(y\\): \\[y_i = \\beta_0+\\beta_1\\times x_i\\]\nКак изменится \\(y_j\\), если мы увеличем \\(x_i + 1 = x_j\\)? \\[y_j = \\beta_0+\\beta_1\\times x_j\\]\n\\[y_j - y_i = \\beta_0+\\beta_1\\times x_j - (\\beta_0+\\beta_1\\times x_i)  = \\beta_1(x_j - x_i)\\]\nТ. е. \\(y\\) увеличится на \\(\\beta_1\\) , если \\(x\\) увеличится на 1. Что же будет с логарифмированными переменными? Как изменится \\(y_j\\), если мы увеличем \\(x_i + 1 = x_j\\)?\n\\[\\log(y_j) - \\log(y_i) = \\beta_1\\times (\\log(x_j) - \\log(x_i))\\]\n\\[\\log\\left(\\frac{y_j}{y_i}\\right) = \\beta_1\\times \\log\\left(\\frac{x_j}{x_i}\\right) = \\log\\left(\\left(\\frac{x_j}{x_i}\\right) ^ {\\beta_1}\\right)\\]\n\\[\\frac{y_j}{y_i}= \\left(\\frac{x_j}{x_i}\\right) ^ {\\beta_1}\\]\nТ. е. \\(y\\) увеличится на \\(\\beta_1\\) процентов, если \\(x\\) увеличится на 1 процент.\nЛогарифмирование — не единственный вид траснформации:\n\nтрансформация Тьюки\n\n\nshiny::runGitHub(\"agricolamz/tukey_transform\")\n\n\n\n\n\n\n\n\n\n\n\nтрансформация Бокса — Кокса\n…\n\n\n\n\n\n\n\nВ датасет собрана частотность разных лемм на основании корпуса НКРЯ (Ляшевская and Шаров 2009) (в датасете только значения больше ipm &gt; 10). Известно, что частотность слова связана с рангом слова (см. закон Ципфа). Постройте переменную ранга внутри каждой из частей речи и визуализируйте связь ранга и логорифма частотности с разбивкой по частям речи. Какие части речи так и не приобрели после трансформации “приемлимую” линейную форму? (я насчитал 5 таких)\n\n\n\n\n\n\n\n\na\n\n\n\n\nadv\n\n\n\n\nadvpro\n\n\n\n\nanum\n\n\n\n\napro\n\n\n\n\nconj\n\n\n\n\nintj\n\n\n\n\nnum\n\n\n\n\npart\n\n\n\n\npr\n\n\n\n\ns\n\n\n\n\ns.PROP\n\n\n\n\nspro\n\n\n\n\nv",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Ограничения на применение регрессии</span>"
    ]
  },
  {
    "objectID": "10-advanced-regression.html#нормальность-распределение-остатков",
    "href": "10-advanced-regression.html#нормальность-распределение-остатков",
    "title": "10  Ограничения на применение регрессии",
    "section": "10.6 Нормальность распределение остатков",
    "text": "10.6 Нормальность распределение остатков\nЛинейная регрессия предполагает нормальность распределения остатков. Когда связь не линейна, то остатки тоже будут распределены не нормально.\nМожно смотреть на первый график используя функцию plot(m1) — график остатков. Интерпретаций этого графика достаточно много (см. статью про это).\n\n10.6.1 Quantile-Quantile Plots (aka Q-Q plot)\nМожно смотреть на qqplot:\n\ntibble(res = m1$residuals) |&gt; \n  ggplot(aes(res))+\n  geom_histogram(aes(y = ..density..))+\n  stat_function(fun = dnorm, args = list(mean = 0, sd = sd(m1$residuals)), color = \"red\")\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n\n\n\n\n\n\ntibble(residuals = m1$residuals) |&gt; \n  ggplot(aes(sample = residuals))+\n  stat_qq()+\n  stat_qq_line()\n\n\n\n\n\n\n\ntibble(res = m2$residuals) |&gt; \n  ggplot(aes(res))+\n  geom_histogram(aes(y = ..density..))+\n  stat_function(fun = dnorm, args = list(mean = 0, sd = sd(m2$residuals)), color = \"red\")\n\n\n\n\n\n\n\ntibble(residuals = m2$residuals) |&gt; \n  ggplot(aes(sample = residuals))+\n  stat_qq()+\n  stat_qq_line()\n\n\n\n\n\n\n\ntibble(res = m3$residuals) |&gt; \n  ggplot(aes(res))+\n  geom_histogram(aes(y = ..density..))+\n  stat_function(fun = dnorm, args = list(mean = 0, sd = sd(m3$residuals)), color = \"red\")\n\n\n\n\n\n\n\ntibble(residuals = m3$residuals) |&gt; \n  ggplot(aes(sample = residuals))+\n  stat_qq()+\n  stat_qq_line()\n\n\n\n\n\n\n\n\nИдея q-q plot заключается в визуальном анализе множества точек каждая из которых соотносится с квантилями анализируемого распределения и квантилями теоретического распределения. В анализируемом нами векторе m1$residuals 100 наблюдений.\n\nquantiles &lt;- seq(0, 1, length.out = 102)[-c(1, 102)]\nquantiles\n\n  [1] 0.00990099 0.01980198 0.02970297 0.03960396 0.04950495 0.05940594\n  [7] 0.06930693 0.07920792 0.08910891 0.09900990 0.10891089 0.11881188\n [13] 0.12871287 0.13861386 0.14851485 0.15841584 0.16831683 0.17821782\n [19] 0.18811881 0.19801980 0.20792079 0.21782178 0.22772277 0.23762376\n [25] 0.24752475 0.25742574 0.26732673 0.27722772 0.28712871 0.29702970\n [31] 0.30693069 0.31683168 0.32673267 0.33663366 0.34653465 0.35643564\n [37] 0.36633663 0.37623762 0.38613861 0.39603960 0.40594059 0.41584158\n [43] 0.42574257 0.43564356 0.44554455 0.45544554 0.46534653 0.47524752\n [49] 0.48514851 0.49504950 0.50495050 0.51485149 0.52475248 0.53465347\n [55] 0.54455446 0.55445545 0.56435644 0.57425743 0.58415842 0.59405941\n [61] 0.60396040 0.61386139 0.62376238 0.63366337 0.64356436 0.65346535\n [67] 0.66336634 0.67326733 0.68316832 0.69306931 0.70297030 0.71287129\n [73] 0.72277228 0.73267327 0.74257426 0.75247525 0.76237624 0.77227723\n [79] 0.78217822 0.79207921 0.80198020 0.81188119 0.82178218 0.83168317\n [85] 0.84158416 0.85148515 0.86138614 0.87128713 0.88118812 0.89108911\n [91] 0.90099010 0.91089109 0.92079208 0.93069307 0.94059406 0.95049505\n [97] 0.96039604 0.97029703 0.98019802 0.99009901\n\nx &lt;- qnorm(seq(0, 1, length.out = length(m1$residuals)+2))\nx\n\n  [1]        -Inf -2.33007892 -2.05785598 -1.88517703 -1.75530050 -1.64967268\n  [7] -1.55977999 -1.48097265 -1.41041953 -1.34626267 -1.28721373 -1.23234086\n [13] -1.18094704 -1.13249653 -1.08656811 -1.04282424 -1.00098992 -0.96083793\n [19] -0.92217818 -0.88484984 -0.84871553 -0.81365681 -0.77957077 -0.74636734\n [25] -0.71396710 -0.68229963 -0.65130211 -0.62091817 -0.59109698 -0.56179247\n [31] -0.53296269 -0.50456929 -0.47657700 -0.44895330 -0.42166805 -0.39469322\n [37] -0.36800261 -0.34157166 -0.31537724 -0.28939747 -0.26361161 -0.23799988\n [43] -0.21254334 -0.18722382 -0.16202378 -0.13692623 -0.11191464 -0.08697288\n [49] -0.06208512 -0.03723576 -0.01240937  0.01240937  0.03723576  0.06208512\n [55]  0.08697288  0.11191464  0.13692623  0.16202378  0.18722382  0.21254334\n [61]  0.23799988  0.26361161  0.28939747  0.31537724  0.34157166  0.36800261\n [67]  0.39469322  0.42166805  0.44895330  0.47657700  0.50456929  0.53296269\n [73]  0.56179247  0.59109698  0.62091817  0.65130211  0.68229963  0.71396710\n [79]  0.74636734  0.77957077  0.81365681  0.84871553  0.88484984  0.92217818\n [85]  0.96083793  1.00098992  1.04282424  1.08656811  1.13249653  1.18094704\n [91]  1.23234086  1.28721373  1.34626267  1.41041953  1.48097265  1.55977999\n [97]  1.64967268  1.75530050  1.88517703  2.05785598  2.33007892         Inf\n\nx &lt;- x[-c(1, length(m1$residuals)+2)]\ny &lt;- sort(m1$residuals)\n\ntibble(x, y) |&gt; \n  ggplot(aes(x, y))+\n  geom_point()\n\n\n\n\n\n\n\n\nЛинию, показывающая “теоретические ожидания”, обычно проводят через первый и третью квартили, т. е. через точки со следующими координатами:\n\ntibble(x, y) |&gt; \n  ggplot(aes(x, y))+\n  geom_point()+\n  annotate(geom = \"point\", color = \"red\", size = 3,\n           x = quantile(x, c(0.25, 0.75)), \n           y = quantile(y, c(0.25, 0.75)))\n\n\n\n\n\n\n\n# я ленивый и вместо того чтобы выводить формулу прямой, я опять запускаю регрессию\nline_coefficients &lt;- lm(y~x, data = tibble(x = quantile(x, c(0.25, 0.75)),\n                                           y = quantile(y, c(0.25, 0.75))))\n\ncoef(line_coefficients)\n\n(Intercept)           x \n  -1.760719  126.943082 \n\ntibble(x, y) |&gt; \n  ggplot(aes(x, y))+\n  geom_point()+\n  annotate(geom = \"point\", color = \"red\", size = 3,\n           x = quantile(x, c(0.25, 0.75)), \n           y = quantile(y, c(0.25, 0.75)))+\n  geom_abline(intercept = coef(line_coefficients)[1],\n              slope = coef(line_coefficients)[2],\n              linetype = 2, color = \"red\")\n\n\n\n\n\n\n\n\nДавайте поиграем с разными распределениями:\n\ny &lt;- rnorm(100, mean = 50, sd = 100) |&gt; sort()\nx &lt;- qnorm(seq(0, 1, length.out = length(y)+2))\nx &lt;- x[-c(1, length(m1$residuals)+2)]\n\nline_coefficients &lt;- lm(y~x, data = tibble(x = quantile(x, c(0.25, 0.75)),\n                                           y = quantile(y, c(0.25, 0.75))))\n\ntibble(x, y) |&gt; \n  ggplot(aes(x, y))+\n  geom_point()+\n  geom_abline(intercept = coef(line_coefficients)[1],\n              slope = coef(line_coefficients)[2],\n              linetype = 2, color = \"red\")\n\n\n\n\n\n\n\ny &lt;- rt(100, df = 20) |&gt; sort()\nx &lt;- qnorm(seq(0, 1, length.out = length(y)+2))\nx &lt;- x[-c(1, length(m1$residuals)+2)]\n\nline_coefficients &lt;- lm(y~x, data = tibble(x = quantile(x, c(0.25, 0.75)),\n                                           y = quantile(y, c(0.25, 0.75))))\n\ntibble(x, y) |&gt; \n  ggplot(aes(x, y))+\n  geom_point()+\n  geom_abline(intercept = coef(line_coefficients)[1],\n              slope = coef(line_coefficients)[2],\n              linetype = 2, color = \"red\")\n\n\n\n\n\n\n\ny &lt;- rlnorm(100, meanlog = 1, sdlog = 1) |&gt; sort()\nx &lt;- qnorm(seq(0, 1, length.out = length(y)+2))\nx &lt;- x[-c(1, length(m1$residuals)+2)]\n\nline_coefficients &lt;- lm(y~x, data = tibble(x = quantile(x, c(0.25, 0.75)),\n                                           y = quantile(y, c(0.25, 0.75))))\n\ntibble(x, y) |&gt; \n  ggplot(aes(x, y))+\n  geom_point()+\n  geom_abline(intercept = coef(line_coefficients)[1],\n              slope = coef(line_coefficients)[2],\n              linetype = 2, color = \"red\")\n\n\n\n\n\n\n\ny &lt;- rexp(100, rate = 3) |&gt; sort()\nx &lt;- qnorm(seq(0, 1, length.out = length(y)+2))\nx &lt;- x[-c(1, length(m1$residuals)+2)]\n\nline_coefficients &lt;- lm(y~x, data = tibble(x = quantile(x, c(0.25, 0.75)),\n                                           y = quantile(y, c(0.25, 0.75))))\n\ntibble(x, y) |&gt; \n  ggplot(aes(x, y))+\n  geom_point()+\n  geom_abline(intercept = coef(line_coefficients)[1],\n              slope = coef(line_coefficients)[2],\n              linetype = 2, color = \"red\")\n\n\n\n\n\n\n\n\nБолее того, ожидаемые квантили теоретического распределения могут быть отличны от нормального:\n\ny &lt;- rexp(100, rate = 3) |&gt; sort()\nx &lt;- qexp(seq(0, 1, length.out = length(y)+2), rate = 3)\nx &lt;- x[-c(1, length(m1$residuals)+2)]\n\nline_coefficients &lt;- lm(y~x, data = tibble(x = quantile(x, c(0.25, 0.75)),\n                                           y = quantile(y, c(0.25, 0.75))))\n\ntibble(x, y) |&gt; \n  ggplot(aes(x, y))+\n  geom_point()+\n  geom_abline(intercept = coef(line_coefficients)[1],\n              slope = coef(line_coefficients)[2],\n              linetype = 2, color = \"red\")",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Ограничения на применение регрессии</span>"
    ]
  },
  {
    "objectID": "10-advanced-regression.html#гетероскидастичность",
    "href": "10-advanced-regression.html#гетероскидастичность",
    "title": "10  Ограничения на применение регрессии",
    "section": "10.7 Гетероскидастичность",
    "text": "10.7 Гетероскидастичность\nРаспределение остатков непостоянно (т.е. не гомоскидастичны):\n\nldt |&gt; \n  ggplot(aes(Mean_RT, Freq))+\n  geom_point()+\n  theme_bw()\n\n\n\n\n\n\n\n\nТоже решается преобразованием данных.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Ограничения на применение регрессии</span>"
    ]
  },
  {
    "objectID": "10-advanced-regression.html#мультиколлинеарность",
    "href": "10-advanced-regression.html#мультиколлинеарность",
    "title": "10  Ограничения на применение регрессии",
    "section": "10.8 Мультиколлинеарность",
    "text": "10.8 Мультиколлинеарность\nЛинейная связь между некоторыми предикторами в модели.\n\nкорреляционная матрица\nVIF (Variance inflation factor), car::vif()\n\nVIF = 1 (Not correlated)\n1 &lt; VIF &lt; 5 (Moderately correlated)\nVIF &gt;=5 (Highly correlated)",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Ограничения на применение регрессии</span>"
    ]
  },
  {
    "objectID": "10-advanced-regression.html#независимость-наблюдений",
    "href": "10-advanced-regression.html#независимость-наблюдений",
    "title": "10  Ограничения на применение регрессии",
    "section": "10.9 Независимость наблюдений",
    "text": "10.9 Независимость наблюдений\nНаблюдения должны быть независимы. В ином случае нужно использовать модель со смешанными эффектами.\n\n10.9.1 Линейная модель со смешанными эффектами\nВ качестве примера мы попробуем поиграть с законом Хердана-Хипса, описывающий взаимосвязь количества уникальных слов в тексте в зависимости от длины текста. В датасете собраны некоторые корпуса Universal Dependencies (Zeman et al. 2020) и некоторые числа, посчитанные на их основании:\n\nud &lt;- read_csv(\"https://raw.githubusercontent.com/agricolamz/2021_da4l/master/data/ud_corpora.csv\")\n\nud |&gt; \n  ggplot(aes(n_words, n_tokens))+\n  geom_point()+\n  facet_wrap(~corpus, scale = \"free\")+\n  geom_smooth(method = \"lm\", se = FALSE)+\n  labs(x = \"количество слов\", \n       y = \"количество уникальных слов\",\n       caption = \"данные корпусов Universal Dependencies\")\n\n\n\n\n\n\n\n\nСвязь между переменными безусловно линейная, однако в разных корпусах представлена разная перспектива: для каких-то корпусов, видимо, тексты специально нарезались, так что тексты таких корпусов содержат от 30-40 до 50-80 слов, а какие-то оставались не тронутыми. Чтобы показать, что связь есть, нельзя просто “слить” все наблюдения в один котел (см. парадокс Симпсона), так как это нарушит предположение регрессии о независимости наблюдений. Мы не можем включить переменную corpus в качестве dummy-переменной: тогда один из корпусов попадет в интерсепт (станет своего рода базовым уровенем), а остальные будут от него отсчитываться. К тому же не очень понятно, как работать с новыми данными из других корпусов: ведь мы хотим предсказывать значения обобщенно, вне зависимости от корпуса.\nПри моделировании при помощи моделей со случайными эффектами различают:\n\nосновные эффекты – это те связи, которые нас интересуют, независимые переменные (количество слов, количество уникальных слов);\nслучайные эффекты – это те переменные, которые создают группировку в данных (корпус).\n\nВ результате моделирования появляется обобщенная модель, которая игнорирует группировку, а потом для каждого значения случайного эффекта генерируется своя регрессия, отсчитывая от обобщенной модели как от базового уровня.\nРассмотрим простейший случай:\n\nlibrary(lme4)\nlibrary(lmerTest)\n\nfit1 &lt;- lmer(n_tokens~n_words+(1|corpus), data = ud)\nsummary(fit1)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: n_tokens ~ n_words + (1 | corpus)\n   Data: ud\n\nREML criterion at convergence: 10321.5\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-7.5271 -0.4947  0.0354  0.5282  8.6350 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n corpus   (Intercept) 240.608  15.512  \n Residual               8.844   2.974  \nNumber of obs: 2046, groups:  corpus, 6\n\nFixed effects:\n              Estimate Std. Error         df t value Pr(&gt;|t|)    \n(Intercept) -4.528e+00  6.353e+00  4.959e+00  -0.713    0.508    \nn_words      8.279e-01  4.418e-03  1.993e+03 187.415   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n        (Intr)\nn_words -0.079\n\nud |&gt; \n  mutate(predicted = predict(fit1)) |&gt; \n  ggplot(aes(n_words, n_tokens))+\n  geom_point()+\n  facet_wrap(~corpus, scale = \"free\")+\n  geom_line(aes(y = predicted), color = \"red\") +\n  labs(x = \"количество слов\", \n       y = \"количество уникальных слов\",\n       caption = \"данные корпусов Universal Dependencies\")\n\n\n\n\n\n\n\n\nМожно посмотреть на предсказания модели (основные эффекты):\n\nlibrary(ggeffects)\nggeffect(fit1) |&gt; \n  plot()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nВизуализируйте полученные модели при помощи функции plot(). Какие ограничения на применение линейной регрессии нарушается в наших моделях?\n\n\n\n\nplot(fit1)\n\n\n\n\n\n\n\n\nВ данном случае мы предполагаем, что случайный эффект имеет случайный свободный член. Т.е. все получающиеся линии параллельны, так как имеют общий угловой коэффициент. Можно допустить большую свободу и сделать так, чтобы в случайном эффекте были не только интерсепт, но и свободный член:\n\nfit2 &lt;- lmer(n_tokens~n_words+(1+n_words|corpus), data = ud)\nsummary(fit2)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: n_tokens ~ n_words + (1 + n_words | corpus)\n   Data: ud\n\nREML criterion at convergence: 10275.2\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-7.8337 -0.5003  0.0293  0.5172  8.8405 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr \n corpus   (Intercept) 4.465751 2.11323       \n          n_words     0.009532 0.09763  -1.00\n Residual             8.693060 2.94840       \nNumber of obs: 2046, groups:  corpus, 6\n\nFixed effects:\n            Estimate Std. Error      df t value Pr(&gt;|t|)    \n(Intercept)  3.23103    0.88937 2.21775   3.633   0.0582 .  \nn_words      0.80323    0.04005 4.10414  20.056 2.99e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n        (Intr)\nn_words -0.988\noptimizer (nloptwrap) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')\n\nud |&gt; \n  mutate(predicted = predict(fit2)) |&gt; \n  ggplot(aes(n_words, n_tokens))+\n  geom_point()+\n  facet_wrap(~corpus, scale = \"free\")+\n  geom_line(aes(y = predicted), color = \"red\") +\n  labs(x = \"количество слов\", \n       y = \"количество уникальных слов\",\n       caption = \"данные корпусов Universal Dependencies\")\n\n\n\n\n\n\n\n\nМожно посмотреть на предсказания модели (основные эффекты):\n\nggeffect(fit2) |&gt; \n  plot()\n\n\n\n\n\n\n\n\nНарушения все те же:\n\nplot(fit2)\n\n\n\n\n\n\n\n\nПри желании мы можем также построить модель, в которой в случайном эффекте будет лишь угловой коэффициент, а свободный член будет фиксированным:\n\nfit3 &lt;- lmer(n_tokens~n_words+(0+n_words|corpus), data = ud)\nsummary(fit3)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: n_tokens ~ n_words + (0 + n_words | corpus)\n   Data: ud\n\nREML criterion at convergence: 10280.9\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-7.8107 -0.4933  0.0315  0.5227  8.8209 \n\nRandom effects:\n Groups   Name    Variance Std.Dev.\n corpus   n_words 0.004023 0.06343 \n Residual         8.717996 2.95263 \nNumber of obs: 2046, groups:  corpus, 6\n\nFixed effects:\n             Estimate Std. Error        df t value Pr(&gt;|t|)    \n(Intercept) 2.648e+00  2.163e-01 2.044e+03   12.24  &lt; 2e-16 ***\nn_words     8.043e-01  2.615e-02 5.162e+00   30.76 4.77e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n        (Intr)\nn_words -0.132\n\nud |&gt; \n  mutate(predicted = predict(fit3)) |&gt; \n  ggplot(aes(n_words, n_tokens))+\n  geom_point()+\n  facet_wrap(~corpus, scale = \"free\")+\n  geom_line(aes(y = predicted), color = \"red\") +\n  labs(x = \"количество слов\", \n       y = \"количество уникальных слов\",\n       caption = \"данные корпусов Universal Dependencies\")\n\n\n\n\n\n\n\n\nЛинии получились очень похожими, но разными:\n\nМожно посмотреть на предсказания модели (основные эффекты):\n\nggeffect(fit3) |&gt; \n  plot()\n\n\n\n\n\n\n\n\nНарушения все те же:\n\nplot(fit3)\n\n\n\n\n\n\n\n\nСравним полученные модели:\n\nanova(fit3, fit2, fit1)\n\n\n  \n\n\n\n\n\n\n\n\n\nПостройте модель со случайными угловым коэффициентом и свободным членом, устранив проблему, которую вы заметили в прошлом задании.\n\n\n\n\n\n\n\n\n\nПользуясь знаниями из предыдущих заданий, смоделируйте связь количества слов и количества существительных. С какими проблемами вы столкнулись?\n\n\n\n\n\n\n\nZeman, D., J. Nivre, Mitchell Abrams, Elia Ackermann, Noëmi Aepli, Hamid Aghaei, Željko Agić, et al. 2020. “Universal Dependencies 2.7.” http://hdl.handle.net/11234/1-3424.\n\n\nЛяшевская, О. Н., and С. А. Шаров. 2009. Частотный Словарь Современного Русского Языка: На Материалах Национального Корпуса Русского Языка. Азбуковник.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Ограничения на применение регрессии</span>"
    ]
  },
  {
    "objectID": "10-advanced-regression.html#footnotes",
    "href": "10-advanced-regression.html#footnotes",
    "title": "10  Ограничения на применение регрессии",
    "section": "",
    "text": "Как и в других функциях, вычисляющих описательную статистику (mean(), median(), max(), min() и др.), функция var() (и все остальные функции, которые мы будем обсуждать sd(), cov()) возвращают NA, если в векторе есть пропущенные значения. Чтобы изменить это поведение, нужно добавить аргумент na.rm = TRUE.↩︎",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Ограничения на применение регрессии</span>"
    ]
  },
  {
    "objectID": "11-introduction_to_bayesian_regression_with_brms.html",
    "href": "11-introduction_to_bayesian_regression_with_brms.html",
    "title": "11  Байесовский регрессионный анализ",
    "section": "",
    "text": "11.1 Основы регрессионного анализа\nКогда мы используем регрессионный анализ, мы пытаемся оценить два параметра:\n\\[y_i = \\hat{\\beta_0} + \\hat{\\beta_1}\\times x_i + \\epsilon_i\\]\nПричем, иногда мы можем один или другой параметр считать равным нулю.\nПри этом, вне зависимости от статистической школы, у регрессии есть свои ограничения на применение:",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Байесовский регрессионный анализ</span>"
    ]
  },
  {
    "objectID": "11-introduction_to_bayesian_regression_with_brms.html#основы-регрессионного-анализа",
    "href": "11-introduction_to_bayesian_regression_with_brms.html#основы-регрессионного-анализа",
    "title": "11  Байесовский регрессионный анализ",
    "section": "",
    "text": "свободный член (intercept) – значение \\(y\\) при \\(x = 0\\);\nугловой коэффициент (slope) – изменение \\(y\\) при изменении \\(x\\) на одну единицу.\n\n\n\n\n\nлинейность связи между \\(x\\) и \\(y\\);\nнормальность распределение остатков \\(\\epsilon_i\\);\nгомоскидастичность — равномерность распределения остатков на всем протяжении \\(x\\);\nнезависимость переменных;\nнезависимость наблюдений друг от друга.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Байесовский регрессионный анализ</span>"
    ]
  },
  {
    "objectID": "11-introduction_to_bayesian_regression_with_brms.html#brms",
    "href": "11-introduction_to_bayesian_regression_with_brms.html#brms",
    "title": "11  Байесовский регрессионный анализ",
    "section": "11.2 brms",
    "text": "11.2 brms\nДля анализа возьмем датасет, который я составил из UD-корпусов и попробуем смоделировать связь между количеством слов в тексте и количеством уникальных слов (закон Хердана-Хипса).\n\nud &lt;- read_csv(\"https://raw.githubusercontent.com/agricolamz/udpipe_count_n_words_and_tokens/master/filtered_dataset.csv\")\nglimpse(ud)\n\nRows: 20,705\nColumns: 5\n$ doc_id      &lt;chr&gt; \"KR1d0052_001\", \"KR1d0052_002\", \"KR1d0052_003\", \"KR1d0052_…\n$ n_words     &lt;dbl&gt; 3516, 2131, 4927, 4884, 4245, 5027, 3406, 2202, 2673, 2300…\n$ n_tokens    &lt;dbl&gt; 842, 546, 869, 883, 737, 1085, 494, 443, 573, 578, 660, 87…\n$ language    &lt;chr&gt; \"Classical_Chinese\", \"Classical_Chinese\", \"Classical_Chine…\n$ corpus_code &lt;chr&gt; \"Kyoto\", \"Kyoto\", \"Kyoto\", \"Kyoto\", \"Kyoto\", \"Kyoto\", \"Kyo…\n\n\nДля начала, нарушим кучу ограничений на применение регрессии и смоделируем модель для вот таких вот данных, взяв только тексты меньше 1500 слов:\n\nud |&gt;\n  filter(n_words &lt; 1500) -&gt;\n  ud\n\nud |&gt; \n  ggplot(aes(n_words, n_tokens))+\n  geom_point()\n\n\n\n\n\n\n\n\n\n11.2.1 Модель только со свободным членом\n\nlibrary(brms)\nparallel::detectCores()\n\n[1] 16\n\nn_cores &lt;- 15 # parallel::detectCores() - 1\n\nget_prior(n_tokens ~ 1, \n          data = ud)\n\n\n  \n\n\n\nВот модель с встроенными априорными распределениями:\n\nfit_intercept &lt;- brm(n_tokens ~ 1, \n                     data = ud,\n                     cores = n_cores, \n                     refresh = 0, \n                     silent = TRUE)\n\nПри желании встроенные априорные распределения можно не использовать и вставлять в аргумент prior априорные распределения по вашему желанию.\n\nfit_intercept\n\n Family: gaussian \n  Links: mu = identity \nFormula: n_tokens ~ 1 \n   Data: ud (Number of observations: 20282) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept   135.63      0.83   134.00   137.27 1.00     2354     2220\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma   121.76      0.62   120.56   122.98 1.00     4009     2905\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nplot(fit_intercept)\n\n\n\n\n\n\n\n\n\n\n11.2.2 Проверка сходимости модели\nВы посторили регрессию, и самый простой способ проверить сходимость это визуально посмотреть на цепи:\n\n\n\n\n\n\n\n\n\nДальнейший фрагемнт взят из документации Stan (Stan — это один из популярнейших сэмплеров для MCMC и многого другого, который используется в brms).\n\n11.2.2.1 R-hat\nR-hat – это диагностика сходимости, которая сравнивает оценку модели, которая получается внутри цепи и между цепями (введена в (Gelman and Rubin 1992)). Если цепи не перемешались (например, если нет согласия внутри цепи и между цепями) R-hat будет иметь значение больше 1. Собственно для этого рекоммендуется запускать по-крайней мере 4 цепи. Stan возвращает что-то более сложное, что называется maximum of rank normalized split-R-hat и rank normalized folded-split-R-hat, однако мы не будем в этом разбиратсья, отсылаю к статье или к посту в блоге одного из авторов с объяснением.\nСамое главное: \\(\\hat{R} \\leq 1.01\\) – значит проблем со сходмиостью цепей не обнаружено.\n\n\n11.2.2.2 Effective sample size (ESS)\nEffective sample size (ESS) — это оценка размера выборки достаточной для того, чтобы получить результат такой же точности при помощи случайной выборки из генеральной совокупности. Эту меру используют для оценки размера выборки, в анализе временных рядов и в байесовской статистике. Так как в байесовской статистике мы используем сэмпл из апостериорного распределения для статистического вывода, а значения в цепях коррелируют, то ESS пытается оценить, сколько нужно независимых сэмплов, чтобы получить такую же точность, что и результат MCMC. Чем выше ESS – тем лучше.\nStan возвращает две меры:\n\nBulk-ESS\n\nRoughly speaking, the effective sample size (ESS) of a quantity of interest captures how many independent draws contain the same amount of information as the dependent sample obtained by the MCMC algorithm. Clearly, the higher the ESS the better. Stan uses R-hat adjustment to use the between-chain information in computing the ESS. For example, in case of multimodal distributions with well-separated modes, this leads to an ESS estimate that is close to the number of distinct modes that are found.\nBulk-ESS refers to the effective sample size based on the rank normalized draws. This does not directly compute the ESS relevant for computing the mean of the parameter, but instead computes a quantity that is well defined even if the chains do not have finite mean or variance. Overall bulk-ESS estimates the sampling efficiency for the location of the distribution (e.g. mean and median).\nOften quite smaller ESS would be sufficient for the desired estimation accuracy, but the estimation of ESS and convergence diagnostics themselves require higher ESS. We recommend requiring that the bulk-ESS is greater than 100 times the number of chains. For example, when running four chains, this corresponds to having a rank-normalized effective sample size of at least 400.\n\nTail ESS\n\nTail-ESS computes the minimum of the effective sample sizes (ESS) of the 5% and 95% quantiles. Tail-ESS can help diagnosing problems due to different scales of the chains and slow mixing in the tails. See also general information about ESS above in description of bulk-ESS.\n\n\n\n11.2.3 Опять модель только со свободным членом\nДавайте посмотрим на наши данные:\n\n\n\n\n\n\n\n\n\nЕсли вам хочется вспомнить мем. Вот еще один.\n\n\n11.2.4 Модель только с угловым коэффициентом\n\nget_prior(n_tokens ~ n_words+0,\n          data = ud)\n\n\n  \n\n\n\n\nfit_slope &lt;- brm(n_tokens ~ n_words+0, \n                 data = ud,\n                 cores = n_cores,\n                 refresh = 0,\n                 silent = TRUE)\n\n\nfit_slope\n\n Family: gaussian \n  Links: mu = identity \nFormula: n_tokens ~ n_words + 0 \n   Data: ud (Number of observations: 20282) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n        Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nn_words     0.53      0.00     0.53     0.53 1.00     4497     2988\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    42.79      0.22    42.38    43.22 1.00     1218     1363\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nplot(fit_slope)\n\n\n\n\n\n\n\n\nДавайте посмотрим на предсказания модели в том, виде, в каком их может интерпретировать не специалист по байесовской статистике:\n\nlibrary(tidybayes)\n\nud |&gt; \n  add_epred_draws(fit_slope, ndraws = 50) |&gt;  \n  ggplot(aes(n_words, n_tokens))+\n  geom_point(alpha = 0.01)+\n  stat_lineribbon(aes(y = .epred), color = \"red\") \n\n\n\n\n\n\n\n\n\n\n11.2.5 Модель с угловым коэффициентом и свободным членом\n\nget_prior(n_tokens ~ n_words,\n          data = ud)\n\n\n  \n\n\n\n\nfit_slope_intercept &lt;- brm(n_tokens ~ n_words,\n                           data = ud,\n                           cores = n_cores, refresh = 0, silent = TRUE)\n\n\nfit_slope_intercept\n\n Family: gaussian \n  Links: mu = identity \nFormula: n_tokens ~ n_words \n   Data: ud (Number of observations: 20282) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    24.31      0.39    23.55    25.06 1.00     2668     2649\nn_words       0.48      0.00     0.48     0.48 1.00     4923     3016\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    39.06      0.20    38.67    39.45 1.00     2287     1744\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nplot(fit_slope_intercept)\n\n\n\n\n\n\n\nud |&gt; \n  add_epred_draws(fit_slope_intercept, ndraws = 50) |&gt;  \n  ggplot(aes(n_words, n_tokens))+\n  geom_point(alpha = 0.01)+\n  stat_lineribbon(aes(y = .epred), color = \"red\") \n\n\n\n\n\n\n\n\n\n\n11.2.6 Модель со смешанными эффектами\nВ данных есть группировка по языкам, которую мы все это время игнорировали. Давайте сделаем модель со смешанными эффектами:\n\nget_prior(n_tokens ~ n_words+(1|language),\n          data = ud)\n\n\n  \n\n\n\n\nfit_mixed &lt;- brm(n_tokens ~ n_words + (1|language),\n                 data = ud,\n                 cores = n_cores, refresh = 0, silent = TRUE)\n\nWarning: There were 4 divergent transitions after warmup. See\nhttps://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup\nto find out why this is a problem and how to eliminate them.\n\n\nWarning: There were 181 transitions after warmup that exceeded the maximum treedepth. Increase max_treedepth above 10. See\nhttps://mc-stan.org/misc/warnings.html#maximum-treedepth-exceeded\n\n\nWarning: Examine the pairs() plot to diagnose sampling problems\n\n\n\nfit_mixed\n\nWarning: There were 4 divergent transitions after warmup. Increasing\nadapt_delta above 0.8 may help. See\nhttp://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup\n\n\n Family: gaussian \n  Links: mu = identity \nFormula: n_tokens ~ n_words + (1 | language) \n   Data: ud (Number of observations: 20282) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~language (Number of levels: 9) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)    63.16     18.18    38.81   110.63 1.00      627      925\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     5.60     20.62   -33.74    47.11 1.01      734      992\nn_words       0.49      0.00     0.49     0.49 1.00     4031     2561\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    29.28      0.14    29.00    29.57 1.00     2296     2055\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nplot(fit_mixed)\n\n\n\n\n\n\n\nud |&gt; \n  add_epred_draws(fit_mixed, ndraws = 50) |&gt;  \n  ggplot(aes(n_words, n_tokens))+\n  geom_point(alpha = 0.01)+\n  stat_lineribbon(aes(y = .epred), color = \"red\") +\n  facet_wrap(~language)\n\n\n\n\n\n\n\n\nТо, что получилось учитывает общий эффект всех языков: посмотрите на каталанский. Если построить модель по каждому языку, то получится совсем другая картина:\n\nud |&gt; \n  ggplot(aes(n_words, n_tokens))+\n  geom_smooth(method = \"lm\") +\n  geom_point(alpha = 0.3)+\n  facet_wrap(~language)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nВ работе (Coretta 2016) собраны данные длительности исландских гласных. Используя байесовскую регрессию с априорными распределениями по умолчанию, смоделируйте длительность гласного (vowel.dur) в зависимости от аспирированности (aspiration) учитывая эффект носителя. Визуализируйте результаты модели.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCoretta, Stefano. 2016. “Vowel Duration and Aspiration Effects in Icelandic.” University of York.\n\n\nGelman, A., and D. B. Rubin. 1992. “Inference from Iterative Simulation Using Multiple Sequences.” Statistical Science, 457–72.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Байесовский регрессионный анализ</span>"
    ]
  },
  {
    "objectID": "12-introduction_to_logistic.html",
    "href": "12-introduction_to_logistic.html",
    "title": "12  Введение в логистическую регресиию",
    "section": "",
    "text": "12.1 Основы регрессионного анализа\nКогда мы используем регрессионный анализ, мы пытаемся оценить два параметра:\n\\[y_i = \\hat{\\beta_0} + \\hat{\\beta_1}\\times x_i + \\epsilon_i\\]\nПричем, иногда мы можем один или другой параметр считать равным нулю.\nПри этом, вне зависимости от статистической школы, у регрессии есть свои ограничения на применение:",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Введение в логистическую регресиию</span>"
    ]
  },
  {
    "objectID": "12-introduction_to_logistic.html#основы-регрессионного-анализа",
    "href": "12-introduction_to_logistic.html#основы-регрессионного-анализа",
    "title": "12  Введение в логистическую регресиию",
    "section": "",
    "text": "свободный член (intercept) – значение \\(y\\) при \\(x = 0\\);\nугловой коэффициент (slope) – изменение \\(y\\) при изменении \\(x\\) на одну единицу.\n\n\n\n\n\nлинейность связи между \\(x\\) и \\(y\\);\nнормальность распределение остатков \\(\\epsilon_i\\);\nгомоскидастичность — равномерность распределения остатков на всем протяжении \\(x\\);\nнезависимость переменных;\nнезависимость наблюдений друг от друга.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Введение в логистическую регресиию</span>"
    ]
  },
  {
    "objectID": "12-introduction_to_logistic.html#логистическая-регрессия",
    "href": "12-introduction_to_logistic.html#логистическая-регрессия",
    "title": "12  Введение в логистическую регресиию",
    "section": "12.2 Логистическая регрессия",
    "text": "12.2 Логистическая регрессия\nЛогистическая (logit, logistic) и мультиномиальная (multinomial) регрессия применяются в случаях, когда зависимая переменная является категориальной:\n\nс двумя значениями (логистическая регрессия)\nс более чем двумя значениями, упорядоченными в иерархию (порядковая регрессия)\nс более чем двумя значениями (мультиномиальная регрессия)\n\n\n12.2.1 Теория\nМы хотим чего-то такого: \\[\\underbrace{y}_{[-\\infty, +\\infty]}=\\underbrace{\\mbox{β}_0+\\mbox{β}_1\\cdot x_1+\\mbox{β}_2\\cdot x_2 + \\dots +\\mbox{β}_k\\cdot x_k +\\mbox{ε}_i}_{[-\\infty, +\\infty]}\\] Вероятность — отношение количества успехов к общему числу событий: \\[p = \\frac{\\mbox{\\# успехов}}{\\mbox{\\# неудач} + \\mbox{\\# успехов}}, p \\in [0, 1]\\] Шансы — отношение количества успехов к количеству неудач: \\[odds = \\frac{p}{1-p} = \\frac{p\\mbox{(успеха)}}{p\\mbox{(неудачи)}}, odds \\in [0, +\\infty]\\] Натуральный логарифм шансов: \\[\\log(odds) \\in [-\\infty, +\\infty]\\]\nНо, что нам говорит логарифм шансов? Как нам его интерпретировать?\n\ntibble(n = 10,\n       success = 1:9,\n       failure = n - success,\n       prob.1 = success/(success+failure),\n       odds = success/failure,\n       log_odds = log(odds),\n       prob.2 = exp(log_odds)/(1+exp(log_odds)))\n\n\n  \n\n\n\nКак связаны вероятность и логарифм шансов: \\[\\log(odds) = \\log\\left(\\frac{p}{1-p}\\right)\\] \\[p = \\frac{\\exp(\\log(odds))}{1+\\exp(\\log(odds))}\\]\n\n\n\n\n\n\nЛогарифм шансов равен 0.25. Посчитайте вероятность успеха (округлите до 3 знаков после запятой):\n\n\n\n\n\n\n\n\n\n\n\n\n\nКак связаны вероятность и логарифм шансов:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n12.2.2 brms\nВ датасет собрано 19 языков, со следующими переменными:\n\nlanguage — переменная, содержащая язык\ntone — бинарная переменная, обозначающая наличие тонов\nlong_vowels — бинарная переменная, обозначающая наличие долгих гласных\nstress — бинарная переменная, обозначающая наличие ударения\nejectives — бинарная переменная, обозначающая наличие абруптивных\nconsonants — переменная, содержащая информацию о количестве согласных\nvowels — переменная, содержащая информацию о количестве гласных\n\n\nphonological_profiles &lt;- read_csv(\"https://raw.githubusercontent.com/agricolamz/2023_da4l/master/data/phonological_profiles.csv\")\nglimpse(phonological_profiles)\n\nRows: 19\nColumns: 8\n$ language    &lt;chr&gt; \"Turkish\", \"Korean\", \"Tiwi\", \"Liberia Kpelle\", \"Tulu\", \"Ma…\n$ tone        &lt;lgl&gt; FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, TRUE, FALSE, TRUE…\n$ long_vowels &lt;lgl&gt; TRUE, FALSE, FALSE, FALSE, TRUE, FALSE, TRUE, FALSE, TRUE,…\n$ stress      &lt;lgl&gt; TRUE, TRUE, TRUE, FALSE, FALSE, TRUE, FALSE, TRUE, TRUE, F…\n$ ejectives   &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FA…\n$ consonants  &lt;dbl&gt; 25, 21, 22, 22, 24, 20, 22, 24, 15, 18, 17, 8, 26, 28, 30,…\n$ vowels      &lt;dbl&gt; 8, 11, 4, 12, 13, 6, 20, 12, 5, 11, 8, 5, 14, 6, 7, 7, 5, …\n$ area        &lt;chr&gt; \"Eurasia\", \"Eurasia\", \"Australia\", \"Africa\", \"Eurasia\", \"S…\n\nset.seed(42)\nphonological_profiles %&gt;% \n  ggplot(aes(ejectives, consonants))+\n  geom_boxplot(aes(fill = ejectives), show.legend = FALSE, outlier.alpha = 0)+ \n  # по умолчанию боксплот рисует выбросы, outlier.alpha = 0 -- это отключает\n  geom_jitter(size = 3, width = 0.2)\n\n\n\n\n\n\n\n\n\n12.2.2.1 Модель без предиктора\n\nlibrary(brms)\nparallel::detectCores()\n\n[1] 16\n\nn_cores &lt;- 15 # parallel::detectCores() - 1\n\nget_prior(ejectives ~ 1, \n          family = bernoulli(), \n          data = phonological_profiles)\n\n\n  \n\n\n\n\nlogit_0 &lt;- brm(ejectives~1, \n               family = bernoulli(), \n               data = phonological_profiles,\n               cores = n_cores, \n               refresh = 0, \n               silent = TRUE)\n\n\nlogit_0\n\n Family: bernoulli \n  Links: mu = logit \nFormula: ejectives ~ 1 \n   Data: phonological_profiles (Number of observations: 19) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    -0.80      0.50    -1.84     0.13 1.00     1372     1563\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nplot(logit_0)\n\n\n\n\n\n\n\n\nВ нашем датасете 13 языков не имеют абруптивных и 6 имеют, так что число, которое мы видим в коэффициенте можно оценить как \\(log(6/13)=-0.7732\\), что немного отличается от того, что получилось в модели. Мы можем использовать прошлую формулу, чтобы получить вероятность:\n\ncoef &lt;- -0.79\nexp(coef)/(1+exp(coef))\n\n[1] 0.3121687\n\n\n\n\n12.2.2.2 Модель c одним числовым предиктором\n\nlogit_1 &lt;- brm(ejectives~consonants, \n               family = bernoulli(), \n               data = phonological_profiles,\n               cores = n_cores, \n               refresh = 0, \n               silent = TRUE)\n\n\nlogit_1\n\n Family: bernoulli \n  Links: mu = logit \nFormula: ejectives ~ consonants \n   Data: phonological_profiles (Number of observations: 19) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    -15.65      6.99   -31.79    -5.13 1.00     1864     1858\nconsonants     0.60      0.28     0.18     1.24 1.00     1916     1797\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nplot(logit_1)\n\n\n\n\n\n\n\n\n\nlibrary(tidybayes)\n\nphonological_profiles %&gt;% \n  add_epred_draws(logit_1, ndraws = 50) %&gt;%  \n  mutate(ejectives = as.double(ejectives)) %&gt;% \n  ggplot(aes(consonants, ejectives)) +\n  stat_lineribbon(aes(y = .epred))+\n  geom_point(color = \"red\")\n\n\n\n\n\n\n\n\nКартинка кривая, потому что она строится на основе наших немногочисленных данных. Если мы создадим датафрейм с согласными, то график будет более плавный:\n\ntibble(consonants = seq(10, 50, by = 0.1)) %&gt;% \n  add_epred_draws(logit_1, ndraws = 50) %&gt;%  \n  ggplot(aes(consonants, ejectives)) +\n  stat_lineribbon(aes(y = .epred))\n\n\n\n\n\n\n\n\nИмеет смысл смотреть предсказание модели без привязки к данным, используя функцию conditional_effects():\n\nconditional_effects(logit_1,\n                    prob = 0.8,\n                    effects = c(\"consonants\"))\n\n\n\n\n\n\n\n\nКакова вероятность, что в языке с 29 согласными есть абруптивные?\n\\[\\log\\left({\\frac{p}{1-p}}\\right)_i=\\beta_0+\\beta_1\\times consinants_i + \\epsilon_i\\] \\[\\log\\left({\\frac{p}{1-p}}\\right)=-15.74 + 0.60 \\times 29 = 1.66\\] \\[p = \\frac{e^{1.66}}{1+e^{1.66}} = 0.840238\\] Однако в отличие от фриквентистских моделей, predict() на байесовских моделях делает сэмпл из распределений:\n\npredict(logit_1, newdata = data.frame(consonants = 29))\n\n     Estimate Est.Error Q2.5 Q97.5\n[1,]    0.772 0.4195951    0     1\n\npredict(logit_1, newdata = data.frame(consonants = 29))\n\n     Estimate Est.Error Q2.5 Q97.5\n[1,]  0.79275 0.4053869    0     1\n\npredict(logit_1, newdata = data.frame(consonants = 29))\n\n     Estimate Est.Error Q2.5 Q97.5\n[1,]  0.79325 0.4050252    0     1\n\npredict(logit_1, newdata = data.frame(consonants = 29))\n\n     Estimate Est.Error Q2.5 Q97.5\n[1,]  0.79825 0.4013567    0     1\n\n\n\n\n\n\n\n\nИспользуйте датасет с языками и признакми из лекции для построения модели, которая предсказывает наличие ударение в зависимости от количество фонем.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Введение в логистическую регресиию</span>"
    ]
  }
]